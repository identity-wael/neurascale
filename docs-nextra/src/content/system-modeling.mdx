import { Tabs, Callout, Table } from 'nextra/components'

# System Modeling

This section provides detailed system modeling diagrams that illustrate the internal workings of NeuraScale, including data flows, state machines, sequence diagrams, and entity relationships.

## Overview

The system modeling documentation uses industry-standard diagram types to represent:

1. **Flowcharts** - Step-by-step process flows for key operations
2. **Sequence Diagrams** - Time-ordered interactions between system components
3. **State Diagrams** - State transitions for devices, sessions, and processing pipelines
4. **Entity Relationship Diagrams** - Data model relationships and schemas
5. **Component Interaction Diagrams** - Detailed service communication patterns

## Process Flowcharts

### Device Connection Flow

This flowchart illustrates the complete process from device discovery to active data streaming, including error handling and recovery mechanisms.

```mermaid
flowchart TB
    Start([User Initiates Connection])

    Discovery{Device<br/>Discovery Mode}
    USB[USB Device<br/>Detection]
    BLE[BLE Device<br/>Scanning]
    WIFI[WiFi/LSL<br/>Network Scan]

    DeviceList[Display Available<br/>Devices List]

    SelectDevice{User Selects<br/>Device}

    CheckDriver{Driver<br/>Installed?}
    InstallDriver[Install Device<br/>Driver]

    InitConnection[Initialize<br/>Connection]

    Handshake{Protocol<br/>Handshake}

    Configure[Configure Device<br/>Parameters]
    SetSampleRate[Set Sample Rate<br/>250-1000 Hz]
    SetChannels[Select Active<br/>Channels]
    SetFilters[Configure<br/>Filters]

    Impedance{Impedance<br/>Check Required?}
    CheckImpedance[Run Impedance<br/>Check]
    ImpedanceOK{Impedance<br/><10kÎ©?}

    StartBuffer[Initialize<br/>Ring Buffer]

    StartStream[Start Data<br/>Stream]

    Validate{Data<br/>Valid?}

    ProcessData[Process<br/>Incoming Data]
    PublishData[Publish to<br/>Pub/Sub]

    Streaming([Active Streaming])

    Error[Connection<br/>Error]
    Retry{Retry<br/>Connection?}

    Disconnect[Disconnect<br/>Device]

    End([Connection Ended])

    Start --> Discovery
    Discovery --> USB
    Discovery --> BLE
    Discovery --> WIFI

    USB --> DeviceList
    BLE --> DeviceList
    WIFI --> DeviceList

    DeviceList --> SelectDevice

    SelectDevice -->|No Device| End
    SelectDevice -->|Device Selected| CheckDriver

    CheckDriver -->|No| InstallDriver
    CheckDriver -->|Yes| InitConnection
    InstallDriver --> InitConnection

    InitConnection --> Handshake

    Handshake -->|Failed| Error
    Handshake -->|Success| Configure

    Configure --> SetSampleRate
    SetSampleRate --> SetChannels
    SetChannels --> SetFilters
    SetFilters --> Impedance

    Impedance -->|Yes| CheckImpedance
    Impedance -->|No| StartBuffer

    CheckImpedance --> ImpedanceOK
    ImpedanceOK -->|No| Error
    ImpedanceOK -->|Yes| StartBuffer

    StartBuffer --> StartStream
    StartStream --> Validate

    Validate -->|Invalid| Error
    Validate -->|Valid| ProcessData

    ProcessData --> PublishData
    PublishData --> Streaming

    Streaming -->|Data Flow| ProcessData
    Streaming -->|User Disconnect| Disconnect
    Streaming -->|Connection Lost| Error

    Error --> Retry
    Retry -->|Yes| InitConnection
    Retry -->|No| Disconnect

    Disconnect --> End

    style Start fill:#34a853,color:#fff
    style Streaming fill:#4285f4,color:#fff
    style Error fill:#ea4335,color:#fff
    style End fill:#9c27b0,color:#fff
    style ProcessData fill:#fbbc04,color:#000
    style PublishData fill:#fbbc04,color:#000
```

<Tabs items={['Process Steps', 'Error Handling', 'Implementation Details']}>
  <Tabs.Tab>
    **Key Process Steps:**

    1. **Discovery Phase**
       - USB devices detected via serial port enumeration
       - BLE devices found through active scanning
       - WiFi/LSL devices discovered via mDNS broadcast

    2. **Connection Setup**
       - Driver verification and installation if needed
       - Protocol-specific handshake (varies by device type)
       - Connection parameters negotiation

    3. **Configuration**
       - Sample rate selection (250Hz, 500Hz, 1000Hz)
       - Channel mapping and activation
       - Digital filter configuration (notch, bandpass)

    4. **Quality Assurance**
       - Optional impedance checking per channel
       - Signal quality validation
       - Automatic bad channel detection

    5. **Data Streaming**
       - Zero-copy ring buffer initialization
       - Real-time data validation
       - Asynchronous Pub/Sub publishing
  </Tabs.Tab>

  <Tabs.Tab>
    **Error Handling Mechanisms:**

    - **Connection Failures**: Exponential backoff retry strategy
    - **Driver Issues**: Automatic driver download and installation
    - **Protocol Errors**: Fallback to compatible protocol versions
    - **Data Validation**: Packet checksum and timestamp verification
    - **Network Issues**: Automatic reconnection with state preservation
    - **Buffer Overflow**: Backpressure handling and flow control

    **Recovery Strategies:**
    ```python
    # Retry configuration
    MAX_RETRIES = 3
    INITIAL_BACKOFF = 1.0  # seconds
    MAX_BACKOFF = 30.0     # seconds
    BACKOFF_MULTIPLIER = 2.0
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Implementation References:**

    - **Device Discovery**: `neural-engine/src/devices/discovery_service.py`
    - **Connection Manager**: `neural-engine/src/devices/device_manager.py`
    - **Protocol Handlers**: `neural-engine/src/devices/protocols/`
    - **Ring Buffer**: `neural-engine/src/core/ring_buffer.py`
    - **Pub/Sub Client**: `neural-engine/src/messaging/pubsub_client.py`

    **Key Classes:**
    ```python
    class DeviceDiscoveryService:
        async def discover_devices(self) -> List[DeviceInfo]

    class DeviceManager:
        async def connect(self, device_id: str) -> Device
        async def configure(self, device: Device, config: DeviceConfig)
        async def start_streaming(self, device: Device)
    ```
  </Tabs.Tab>
</Tabs>

### Session Management Flow

This flowchart shows the complete lifecycle of a neural recording session from creation to completion.

```mermaid
flowchart TB
    Start([User Creates Session])

    CreateSession[Create Session<br/>Metadata]
    AssignID[Generate<br/>Session UUID]

    SelectDevices{Select<br/>Devices}
    ValidateDevices[Validate Device<br/>Availability]

    ConfigSession[Configure Session<br/>Parameters]
    SetDuration[Set Recording<br/>Duration]
    SetTriggers[Configure<br/>Event Triggers]
    SetStorage[Select Storage<br/>Options]

    InitStorage[Initialize<br/>Storage Backend]
    CreateFiles[Create Data<br/>Files]

    StartRecording[Start<br/>Recording]

    Recording([Active Recording])

    MonitorHealth{Monitor<br/>Session Health}

    PauseOption{User Action}
    Pause[Pause<br/>Recording]
    Resume[Resume<br/>Recording]

    StopRecording[Stop<br/>Recording]

    FinalizeData[Finalize<br/>Data Files]
    GenerateMetadata[Generate Session<br/>Metadata]
    ArchiveSession[Archive<br/>Session]

    End([Session Complete])

    Error[Session<br/>Error]
    Cleanup[Cleanup<br/>Resources]

    Start --> CreateSession
    CreateSession --> AssignID
    AssignID --> SelectDevices

    SelectDevices --> ValidateDevices
    ValidateDevices --> ConfigSession

    ConfigSession --> SetDuration
    SetDuration --> SetTriggers
    SetTriggers --> SetStorage
    SetStorage --> InitStorage

    InitStorage --> CreateFiles
    CreateFiles --> StartRecording
    StartRecording --> Recording

    Recording --> MonitorHealth
    MonitorHealth -->|Healthy| PauseOption
    MonitorHealth -->|Error| Error

    PauseOption -->|Pause| Pause
    PauseOption -->|Stop| StopRecording
    PauseOption -->|Continue| Recording

    Pause --> Resume
    Resume --> Recording

    StopRecording --> FinalizeData
    FinalizeData --> GenerateMetadata
    GenerateMetadata --> ArchiveSession
    ArchiveSession --> End

    Error --> Cleanup
    Cleanup --> End

    style Start fill:#34a853,color:#fff
    style Recording fill:#4285f4,color:#fff
    style Error fill:#ea4335,color:#fff
    style End fill:#9c27b0,color:#fff
```

## Sequence Diagrams

### Device Streaming Sequence

This sequence diagram shows the time-ordered interactions between components during real-time device streaming.

```mermaid
sequenceDiagram
    participant Client
    participant API as API Gateway
    participant Auth as Auth Service
    participant DM as Device Manager
    participant Device as Device (OpenBCI)
    participant RB as Ring Buffer
    participant PS as Pub/Sub
    participant DS as Data Service
    participant WS as WebSocket

    Client->>API: POST /devices/{id}/stream/start
    API->>Auth: Validate JWT Token
    Auth-->>API: Token Valid + User Permissions
    API->>DM: StartStreaming(deviceId, config)

    DM->>Device: Connect()
    Device-->>DM: Connection Established

    DM->>Device: Configure(sampleRate, channels)
    Device-->>DM: Configuration ACK

    DM->>RB: Initialize(size=1MB)
    RB-->>DM: Buffer Ready

    DM->>Device: StartDataStream()
    Device-->>DM: Stream Started

    loop Every 4ms (250Hz)
        Device->>DM: DataPacket(samples, timestamp)
        DM->>DM: Validate Packet
        DM->>RB: Write(data)

        alt Buffer Threshold Reached (1000 samples)
            RB->>PS: PublishBatch(topic="device.data.raw")
            PS-->>DS: Store(data)
            PS-->>WS: Broadcast(data)
            WS-->>Client: Real-time Data
        end
    end

    Client->>API: POST /devices/{id}/stream/stop
    API->>DM: StopStreaming(deviceId)
    DM->>Device: StopDataStream()
    Device-->>DM: Stream Stopped
    DM->>RB: Flush()
    RB->>PS: PublishRemaining()
    DM->>Device: Disconnect()
    Device-->>DM: Disconnected
    DM-->>API: Success
    API-->>Client: 200 OK
```

<Tabs items={['Timing Details', 'Message Formats', 'Error Scenarios']}>
  <Tabs.Tab>
    **Timing Specifications:**

    | Operation | Typical Latency | Max Latency | Notes |
    |-----------|----------------|-------------|--------|
    | JWT Validation | 5-10ms | 50ms | Cached tokens faster |
    | Device Connection | 100-500ms | 2000ms | USB fastest, BLE slowest |
    | Configuration | 20-50ms | 100ms | Depends on parameters |
    | Buffer Init | Less than 1ms | 5ms | Pre-allocated memory |
    | Data Packet | 4ms | 8ms | 250Hz sampling rate |
    | Pub/Sub Publish | 5-10ms | 20ms | Async operation |
    | WebSocket Broadcast | 2-5ms | 10ms | Direct connection |

    **Buffering Strategy:**
    - Ring buffer size: 1MB (holds ~10s of data)
    - Batch threshold: 1000 samples (~4 seconds)
    - Flush interval: 100ms (failsafe)
  </Tabs.Tab>

  <Tabs.Tab>
    **Key Message Formats:**

    ```python
    # Data Packet Structure
    class NeuralDataPacket:
        timestamp: datetime       # UTC timestamp
        device_id: str           # Unique device identifier
        sequence_number: int     # Packet sequence counter
        samples: np.ndarray      # Shape: (n_channels, n_samples)
        sample_rate: int         # Hz (250, 500, 1000)
        channel_mask: int        # Active channels bitmask
        battery_level: float     # 0.0 to 1.0
        signal_quality: List[float]  # Per-channel quality
    ```

    **Pub/Sub Message Format:**
    ```json
    {
        "topic": "device.data.raw",
        "device_id": "openbci_cyton_001",
        "timestamp": "2024-01-15T10:30:45.123Z",
        "data": {
            "packets": ["..."],
            "batch_size": 1000,
            "duration_ms": 4000
        }
    }
    ```

    **WebSocket Frame Format:**
    ```json
    {
        "type": "data",
        "device_id": "openbci_cyton_001",
        "channels": [1.23, 4.56, 7.89],
        "timestamp": 1705317045123,
        "quality": [0.98, 0.99, 0.97]
    }
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Error Handling Sequences:**

    ```mermaid
    sequenceDiagram
        participant Client
        participant DM as Device Manager
        participant Device
        participant PS as Pub/Sub

        Note over Client,PS: Scenario 1: Connection Lost
        Device--xDM: Connection Lost
        DM->>DM: Detect Disconnection
        DM->>PS: Publish("device.disconnected")
        DM->>DM: Attempt Reconnection

        alt Reconnection Successful
            DM->>Device: Connect()
            Device-->>DM: Connected
            DM->>Device: Resume Stream
            DM->>PS: Publish("device.reconnected")
        else Reconnection Failed
            DM->>PS: Publish("device.error")
            DM-->>Client: Error Notification
        end

        Note over Client,PS: Scenario 2: Buffer Overflow
        Device->>DM: DataPacket
        DM->>DM: Buffer Full Check
        DM->>PS: EmergencyFlush()
        DM->>DM: Apply Backpressure
        DM-->>Device: Slow Down Signal
    ```
  </Tabs.Tab>
</Tabs>

## State Diagrams

### Device State Machine

This state diagram shows all possible device states and the transitions between them, including guards and actions.

```mermaid
stateDiagram-v2
    [*] --> Disconnected

    Disconnected --> Connecting: connect()

    Connecting --> Connected: handshake_success
    Connecting --> Error: handshake_failed
    Connecting --> Disconnected: timeout

    Connected --> Configuring: configure()

    Configuring --> Ready: config_applied
    Configuring --> Error: config_failed

    Ready --> Streaming: start_stream()
    Ready --> Disconnected: disconnect()

    Streaming --> Paused: pause()
    Streaming --> Ready: stop_stream()
    Streaming --> Error: stream_error

    Paused --> Streaming: resume()
    Paused --> Ready: stop_stream()

    Error --> Reconnecting: auto_retry [retry_count < 3]
    Error --> Disconnected: manual_reset
    Error --> Failed: auto_retry [retry_count >= 3]

    Reconnecting --> Connected: reconnect_success
    Reconnecting --> Error: reconnect_failed

    Failed --> Disconnected: reset()

    Connected --> Disconnected: connection_lost
    Ready --> Disconnected: connection_lost
    Streaming --> Disconnected: connection_lost
    Paused --> Disconnected: connection_lost

    state Connected {
        [*] --> Idle
        Idle --> CheckingImpedance: check_impedance()
        CheckingImpedance --> Idle: complete
    }

    state Streaming {
        [*] --> Active
        Active --> Buffering: buffer_full
        Buffering --> Active: buffer_flushed
    }
```

<Tabs items={['State Descriptions', 'Transition Events', 'Implementation']}>
  <Tabs.Tab>
    **Device States:**

    | State | Description | Allowed Actions |
    |-------|-------------|-----------------|
    | **Disconnected** | No active connection to device | Connect |
    | **Connecting** | Establishing connection | Cancel |
    | **Connected** | Connected but not configured | Configure, Disconnect, Check Impedance |
    | **Configuring** | Applying device settings | Cancel |
    | **Ready** | Configured and ready to stream | Start Stream, Disconnect |
    | **Streaming** | Actively receiving data | Pause, Stop, Monitor |
    | **Paused** | Streaming suspended | Resume, Stop |
    | **Error** | Recoverable error state | Retry, Reset |
    | **Reconnecting** | Attempting automatic recovery | Cancel |
    | **Failed** | Unrecoverable error | Manual Reset |

    **Composite States:**
    - **Connected**: Contains sub-states for impedance checking
    - **Streaming**: Contains sub-states for buffer management
  </Tabs.Tab>

  <Tabs.Tab>
    **State Transitions:**

    ```python
    # Transition Guards
    class TransitionGuards:
        @staticmethod
        def can_retry(device: Device) -> bool:
            return device.retry_count < MAX_RETRIES

        @staticmethod
        def has_valid_config(device: Device) -> bool:
            return device.validate_configuration()

        @staticmethod
        def buffer_available(device: Device) -> bool:
            return device.buffer.free_space > MIN_BUFFER_SIZE

    # Transition Actions
    class TransitionActions:
        @staticmethod
        async def on_connect(device: Device):
            device.retry_count = 0
            await device.initialize_driver()

        @staticmethod
        async def on_streaming_start(device: Device):
            await device.buffer.initialize()
            await device.start_data_thread()

        @staticmethod
        async def on_error(device: Device, error: Exception):
            await device.log_error(error)
            await device.notify_error_handlers(error)
            device.retry_count += 1
    ```

    **Event Triggers:**
    - User actions: `connect()`, `disconnect()`, `start_stream()`, `stop_stream()`
    - System events: `connection_lost`, `buffer_full`, `data_timeout`
    - Error events: `handshake_failed`, `config_failed`, `stream_error`
  </Tabs.Tab>

  <Tabs.Tab>
    **State Machine Implementation:**

    ```python
    from enum import Enum, auto
    from typing import Optional, Callable

    class DeviceState(Enum):
        DISCONNECTED = auto()
        CONNECTING = auto()
        CONNECTED = auto()
        CONFIGURING = auto()
        READY = auto()
        STREAMING = auto()
        PAUSED = auto()
        ERROR = auto()
        RECONNECTING = auto()
        FAILED = auto()

    class DeviceStateMachine:
        def __init__(self, device_id: str):
            self.device_id = device_id
            self.state = DeviceState.DISCONNECTED
            self.retry_count = 0
            self.state_handlers = {}
            self.transition_callbacks = []

        def register_state_handler(
            self,
            state: DeviceState,
            handler: Callable
        ):
            self.state_handlers[state] = handler

        async def transition_to(
            self,
            new_state: DeviceState,
            event: Optional[str] = None
        ):
            old_state = self.state

            # Validate transition
            if not self._is_valid_transition(old_state, new_state):
                raise InvalidTransitionError(
                    f"Cannot transition from {old_state} to {new_state}"
                )

            # Execute exit actions
            await self._execute_exit_actions(old_state)

            # Update state
            self.state = new_state

            # Execute entry actions
            await self._execute_entry_actions(new_state)

            # Notify callbacks
            for callback in self.transition_callbacks:
                await callback(old_state, new_state, event)
    ```

    **Usage Example:**
    ```python
    # neural-engine/src/devices/state_machine.py
    device_sm = DeviceStateMachine("openbci_001")
    device_sm.register_state_handler(
        DeviceState.STREAMING,
        handle_streaming_state
    )

    await device_sm.transition_to(DeviceState.CONNECTING)
    ```
  </Tabs.Tab>
</Tabs>

## Entity Relationship Diagrams

### Core Data Model

This ER diagram shows the relationships between core entities in the NeuraScale system.

```mermaid
erDiagram
    User ||--o{ Session : creates
    User ||--o{ Device : owns
    User {
        string user_id PK
        string email UK
        string name
        string role
        datetime created_at
        datetime last_login
        boolean is_active
    }

    Device ||--o{ Session : participates_in
    Device ||--o{ DeviceStatus : has
    Device {
        string device_id PK
        string user_id FK
        string device_type
        string serial_number UK
        string firmware_version
        json configuration
        datetime registered_at
        datetime last_seen
    }

    Session ||--o{ Recording : contains
    Session ||--o{ Event : logs
    Session {
        string session_id PK
        string user_id FK
        string name
        datetime start_time
        datetime end_time
        string status
        json metadata
        string storage_path
    }

    Recording ||--o{ DataChunk : stores
    Recording ||--o{ Feature : extracts
    Recording {
        string recording_id PK
        string session_id FK
        string device_id FK
        datetime timestamp
        int sample_rate
        int channel_count
        string data_format
        bigint sample_count
    }

    DataChunk ||--o{ Sample : contains
    DataChunk {
        string chunk_id PK
        string recording_id FK
        int sequence_number
        datetime start_time
        datetime end_time
        blob compressed_data
        string storage_location
        int size_bytes
    }

    Sample {
        bigint sample_id PK
        string chunk_id FK
        int channel
        float value
        int sample_index
        float quality_score
    }

    Feature ||--o{ Classification : produces
    Feature {
        string feature_id PK
        string recording_id FK
        string feature_type
        json feature_vector
        datetime computed_at
        string algorithm_version
    }

    Classification {
        string classification_id PK
        string feature_id FK
        string model_id FK
        string class_label
        float confidence
        json probabilities
        datetime predicted_at
    }

    Event {
        string event_id PK
        string session_id FK
        string event_type
        json event_data
        datetime timestamp
        string severity
    }

    DeviceStatus {
        string status_id PK
        string device_id FK
        float battery_level
        json impedance_values
        float signal_quality
        datetime measured_at
        boolean is_streaming
    }
```

<Tabs items={['Entity Details', 'Relationships', 'Database Schema']}>
  <Tabs.Tab>
    **Core Entities:**

    | Entity | Description | Primary Storage |
    |--------|-------------|-----------------|
    | **User** | System users (researchers, clinicians) | PostgreSQL |
    | **Device** | Neural recording devices | PostgreSQL |
    | **Session** | Recording sessions | PostgreSQL |
    | **Recording** | Continuous data from a device | PostgreSQL + Bigtable |
    | **DataChunk** | Compressed time-series segments | Bigtable |
    | **Sample** | Individual data points | Bigtable |
    | **Feature** | Extracted features from recordings | BigQuery |
    | **Classification** | ML model predictions | BigQuery |
    | **Event** | Session events and markers | PostgreSQL |
    | **DeviceStatus** | Real-time device metrics | Redis + PostgreSQL |

    **Data Volume Estimates:**
    - Users: ~1,000
    - Devices: ~10,000
    - Sessions: ~100,000/month
    - Recordings: ~1M/month
    - Samples: ~100B/month (at 250Hz)
    - Features: ~10M/month
    - Classifications: ~10M/month
  </Tabs.Tab>

  <Tabs.Tab>
    **Relationship Cardinalities:**

    ```sql
    -- One-to-Many Relationships
    User (1) ----< (N) Session
    User (1) ----< (N) Device
    Device (1) ----< (N) Session
    Device (1) ----< (N) DeviceStatus
    Session (1) ----< (N) Recording
    Session (1) ----< (N) Event
    Recording (1) ----< (N) DataChunk
    Recording (1) ----< (N) Feature
    DataChunk (1) ----< (N) Sample
    Feature (1) ----< (N) Classification

    -- Many-to-Many Relationships (via junction tables)
    Device <----> Session (via Recording)
    User <----> Device (ownership can be transferred)
    ```

    **Referential Integrity Rules:**
    - CASCADE DELETE: Session â Recording â DataChunk â Sample
    - RESTRICT DELETE: User with active Sessions
    - SET NULL: Device deletion sets Recording.device_id to NULL
    - NO ACTION: Feature deletion prevents Classification deletion
  </Tabs.Tab>

  <Tabs.Tab>
    **Physical Database Schema:**

    ```sql
    -- PostgreSQL Tables
    CREATE TABLE users (
        user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        email VARCHAR(255) UNIQUE NOT NULL,
        name VARCHAR(255) NOT NULL,
        role VARCHAR(50) NOT NULL CHECK (role IN ('admin', 'researcher', 'clinician')),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_login TIMESTAMP WITH TIME ZONE,
        is_active BOOLEAN DEFAULT true
    );

    CREATE TABLE devices (
        device_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID REFERENCES users(user_id),
        device_type VARCHAR(50) NOT NULL,
        serial_number VARCHAR(255) UNIQUE NOT NULL,
        firmware_version VARCHAR(50),
        configuration JSONB DEFAULT '{}',
        registered_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_seen TIMESTAMP WITH TIME ZONE,
        INDEX idx_user_devices (user_id),
        INDEX idx_device_type (device_type)
    );

    -- Bigtable Schema
    Table: neural_data
    Row Key: {recording_id}#{timestamp}#{channel}
    Column Families:
      - raw: raw_value, quality_score
      - meta: sample_rate, device_id
      - features: fft, psd, entropy

    -- BigQuery Schema
    Dataset: neurascale_analytics
    Tables:
      - features: Partitioned by DATE(computed_at)
      - classifications: Partitioned by DATE(predicted_at)
      - aggregated_metrics: Materialized views for dashboards
    ```
  </Tabs.Tab>
</Tabs>

### Authentication Sequence

This sequence diagram shows the complete authentication and authorization flow using JWT tokens.

```mermaid
sequenceDiagram
    participant Client
    participant API as API Gateway
    participant Auth as Auth Service
    participant DB as PostgreSQL
    participant Redis
    participant KMS as Cloud KMS

    Note over Client,KMS: Initial Login Flow

    Client->>API: POST /auth/login {email, password}
    API->>Auth: ValidateCredentials(email, password)
    Auth->>DB: SELECT user WHERE email = ?
    DB-->>Auth: User record

    Auth->>Auth: VerifyPassword(hash, password)

    alt Invalid Credentials
        Auth-->>API: 401 Unauthorized
        API-->>Client: 401 {error: "Invalid credentials"}
    else Valid Credentials
        Auth->>KMS: GenerateTokenPair()
        KMS->>KMS: Sign JWT with private key
        KMS-->>Auth: {access_token, refresh_token}

        Auth->>Redis: SETEX session:{user_id} TTL=3600
        Auth->>DB: UPDATE user SET last_login = NOW()

        Auth-->>API: TokenPair + User info
        API-->>Client: 200 {tokens, user}
    end

    Note over Client,KMS: Authenticated Request Flow

    Client->>API: GET /devices {Authorization: Bearer <token>}
    API->>Auth: ValidateToken(access_token)

    Auth->>Redis: GET session:{user_id}

    alt Session Not Found
        Auth->>KMS: VerifySignature(token)

        alt Invalid Token
            Auth-->>API: 401 Unauthorized
            API-->>Client: 401 {error: "Invalid token"}
        else Valid Token
            Auth->>Auth: CheckExpiration(token)

            alt Token Expired
                Auth-->>API: 401 {error: "Token expired"}
                API-->>Client: 401 {error: "Token expired"}
            else Token Valid
                Auth->>Redis: SETEX session:{user_id} TTL=3600
                Auth-->>API: User context
                API->>API: Process request
                API-->>Client: 200 {data}
            end
        end
    else Session Found
        Auth-->>API: User context (from cache)
        API->>API: Process request
        API-->>Client: 200 {data}
    end

    Note over Client,KMS: Token Refresh Flow

    Client->>API: POST /auth/refresh {refresh_token}
    API->>Auth: RefreshTokens(refresh_token)

    Auth->>KMS: VerifySignature(refresh_token)
    Auth->>DB: SELECT user WHERE id = ? AND is_active = true

    alt Valid Refresh Token
        Auth->>KMS: GenerateTokenPair()
        KMS-->>Auth: {new_access_token, new_refresh_token}
        Auth->>Redis: SETEX session:{user_id} TTL=3600
        Auth-->>API: New TokenPair
        API-->>Client: 200 {tokens}
    else Invalid Refresh Token
        Auth->>Redis: DEL session:{user_id}
        Auth-->>API: 401 Unauthorized
        API-->>Client: 401 {error: "Invalid refresh token"}
    end
```

<Tabs items={['Token Details', 'Security Features', 'Implementation']}>
  <Tabs.Tab>
    **JWT Token Structure:**

    ```json
    {
      "header": {
        "alg": "RS256",
        "typ": "JWT",
        "kid": "key_id_2024"
      },
      "payload": {
        "sub": "user_uuid",
        "email": "user@example.com",
        "role": "researcher",
        "permissions": ["read:devices", "write:sessions"],
        "iat": 1705320000,
        "exp": 1705323600,
        "iss": "neurascale.io",
        "aud": "neurascale-api"
      },
      "signature": "..."
    }
    ```

    **Token Lifetimes:**
    - Access Token: 1 hour
    - Refresh Token: 30 days
    - Session Cache: 1 hour (sliding window)

    **Token Storage:**
    - Client: Secure storage (HttpOnly cookies or secure local storage)
    - Server: Redis for session caching
    - Keys: Cloud KMS for signing keys
  </Tabs.Tab>

  <Tabs.Tab>
    **Security Measures:**

    1. **Password Security**
       - Argon2id hashing (memory-hard)
       - Salt per password
       - Configurable work factors

    2. **Token Security**
       - RS256 asymmetric signing
       - Key rotation every 90 days
       - Token binding to IP/device

    3. **Rate Limiting**
       - Login: 5 attempts per 15 minutes
       - Token refresh: 10 per hour
       - API calls: 1000 per hour

    4. **Additional Protection**
       - CSRF tokens for web clients
       - Secure headers (HSTS, CSP)
       - Request signing for sensitive operations

    ```python
    # Rate limiting implementation
    @rate_limit(
        key=lambda: f"login:{request.remote_addr}",
        rate="5/15m",
        method="sliding_window"
    )
    async def login(credentials: LoginRequest):
        # Login logic
        pass
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Authentication Service Implementation:**

    ```python
    from datetime import datetime, timedelta
    from typing import Optional, Tuple
    import jwt
    from argon2 import PasswordHasher
    from redis import Redis

    class AuthService:
        def __init__(self, kms_client, db, redis: Redis):
            self.kms = kms_client
            self.db = db
            self.redis = redis
            self.ph = PasswordHasher()

        async def authenticate(
            self,
            email: str,
            password: str
        ) -> Tuple[str, str]:
            # Fetch user
            user = await self.db.fetch_one(
                "SELECT * FROM users WHERE email = ?",
                email
            )

            if not user:
                raise AuthenticationError("Invalid credentials")

            # Verify password
            try:
                self.ph.verify(user.password_hash, password)
            except:
                raise AuthenticationError("Invalid credentials")

            # Generate tokens
            access_token = await self._generate_access_token(user)
            refresh_token = await self._generate_refresh_token(user)

            # Cache session
            await self._cache_session(user.id, {
                "user_id": user.id,
                "email": user.email,
                "role": user.role,
                "permissions": user.permissions
            })

            return access_token, refresh_token

        async def _generate_access_token(self, user) -> str:
            payload = {
                "sub": str(user.id),
                "email": user.email,
                "role": user.role,
                "permissions": user.permissions,
                "iat": datetime.utcnow(),
                "exp": datetime.utcnow() + timedelta(hours=1)
            }

            # Sign with KMS
            return await self.kms.sign_jwt(payload)
    ```
  </Tabs.Tab>
</Tabs>

### Processing Pipeline State Machine

This state diagram shows the states and transitions for the real-time neural data processing pipeline.

```mermaid
stateDiagram-v2
    [*] --> Idle

    Idle --> Initializing: start_pipeline()

    Initializing --> Ready: init_complete
    Initializing --> Failed: init_error

    Ready --> Processing: data_available
    Ready --> Idle: stop_pipeline()

    Processing --> Buffering: buffer_threshold
    Processing --> FeatureExtraction: window_complete
    Processing --> Error: processing_error

    Buffering --> Processing: buffer_flushed
    Buffering --> Error: buffer_overflow

    FeatureExtraction --> Classification: features_ready
    FeatureExtraction --> Error: extraction_error

    Classification --> Publishing: results_ready
    Classification --> Error: classification_error

    Publishing --> Processing: publish_complete
    Publishing --> Error: publish_error

    Error --> Recovery: auto_recover [retry_count < 3]
    Error --> Failed: max_retries_exceeded
    Error --> Ready: manual_reset

    Recovery --> Processing: recovery_success
    Recovery --> Error: recovery_failed

    Failed --> Idle: reset_pipeline()

    Processing --> Ready: pause()
    Ready --> Processing: resume()

    state Processing {
        [*] --> Receiving
        Receiving --> Validating: packet_received
        Validating --> Filtering: valid_data
        Validating --> Dropping: invalid_data
        Filtering --> Windowing: filtered
        Windowing --> [*]: window_ready
        Dropping --> Receiving: continue
    }

    state FeatureExtraction {
        [*] --> Spectral
        Spectral --> Temporal: fft_complete
        Temporal --> Connectivity: stats_complete
        Connectivity --> [*]: features_complete
    }

    state Classification {
        [*] --> Loading
        Loading --> Inferencing: model_ready
        Inferencing --> PostProcessing: raw_predictions
        PostProcessing --> [*]: final_results
    }
```

<Tabs items={['Pipeline States', 'Processing Details', 'Implementation']}>
  <Tabs.Tab>
    **Pipeline States:**

    | State | Description | Processing Rate |
    |-------|-------------|-----------------|
    | **Idle** | No active processing | 0 samples/s |
    | **Initializing** | Loading models, allocating buffers | N/A |
    | **Ready** | Waiting for data | 0 samples/s |
    | **Processing** | Active data processing | 250-1000 samples/s |
    | **Buffering** | Accumulating data for batch | Variable |
    | **FeatureExtraction** | Computing features | ~10 windows/s |
    | **Classification** | ML inference | ~10 predictions/s |
    | **Publishing** | Sending results | Async |
    | **Error** | Recoverable error state | 0 samples/s |
    | **Recovery** | Attempting auto-recovery | N/A |
    | **Failed** | Unrecoverable error | 0 samples/s |

    **Composite State Details:**
    - **Processing**: Receiving â Validating â Filtering â Windowing
    - **FeatureExtraction**: Spectral â Temporal â Connectivity analysis
    - **Classification**: Model loading â Inference â Post-processing
  </Tabs.Tab>

  <Tabs.Tab>
    **Processing Pipeline Stages:**

    ```python
    # Pipeline Configuration
    PIPELINE_CONFIG = {
        "buffer_size": 10000,        # samples
        "window_size": 1000,         # samples (4s at 250Hz)
        "window_overlap": 0.5,       # 50% overlap
        "batch_size": 10,            # windows per batch
        "feature_dims": 128,         # feature vector size
        "model_timeout": 100,        # ms
        "publish_timeout": 50,       # ms
    }

    # Processing Stages
    class ProcessingStages:
        RECEIVING = "receiving"
        VALIDATING = "validating"
        FILTERING = "filtering"
        WINDOWING = "windowing"
        SPECTRAL = "spectral_features"
        TEMPORAL = "temporal_features"
        CONNECTIVITY = "connectivity_features"
        INFERENCE = "ml_inference"
        PUBLISHING = "result_publishing"
    ```

    **Performance Metrics:**
    - Latency per stage: 1-5ms
    - Total pipeline latency: Less than 50ms
    - Throughput: 1000 samples/s per channel
    - Feature extraction: 10ms per window
    - ML inference: 5-10ms per batch
  </Tabs.Tab>

  <Tabs.Tab>
    **Pipeline State Machine Implementation:**

    ```python
    from enum import Enum, auto
    from dataclasses import dataclass
    from typing import Optional, List
    import asyncio

    class PipelineState(Enum):
        IDLE = auto()
        INITIALIZING = auto()
        READY = auto()
        PROCESSING = auto()
        BUFFERING = auto()
        FEATURE_EXTRACTION = auto()
        CLASSIFICATION = auto()
        PUBLISHING = auto()
        ERROR = auto()
        RECOVERY = auto()
        FAILED = auto()

    @dataclass
    class PipelineContext:
        buffer: RingBuffer
        feature_extractor: FeatureExtractor
        classifier: MLClassifier
        publisher: ResultPublisher
        error_count: int = 0
        current_window: Optional[np.ndarray] = None
        features: Optional[np.ndarray] = None
        predictions: Optional[dict] = None

    class ProcessingPipeline:
        def __init__(self, config: dict):
            self.config = config
            self.state = PipelineState.IDLE
            self.context = None
            self._state_handlers = self._setup_handlers()

        async def start(self):
            await self.transition_to(PipelineState.INITIALIZING)

        async def process_data(self, data: np.ndarray):
            if self.state != PipelineState.READY:
                raise InvalidStateError(
                    f"Cannot process data in state {self.state}"
                )

            self.context.buffer.write(data)

            if self.context.buffer.ready_for_window():
                await self.transition_to(PipelineState.PROCESSING)

        async def transition_to(
            self,
            new_state: PipelineState
        ):
            # Execute state transition
            old_state = self.state
            self.state = new_state

            # Call appropriate handler
            handler = self._state_handlers.get(new_state)
            if handler:
                try:
                    await handler()
                except Exception as e:
                    await self._handle_error(e)

        async def _handle_processing(self):
            # Get window from buffer
            window = self.context.buffer.get_window(
                size=self.config["window_size"],
                overlap=self.config["window_overlap"]
            )

            # Validate and filter
            if not self._validate_window(window):
                return await self.transition_to(PipelineState.READY)

            filtered = await self._filter_window(window)
            self.context.current_window = filtered

            # Move to feature extraction
            await self.transition_to(PipelineState.FEATURE_EXTRACTION)
    ```
  </Tabs.Tab>
</Tabs>

### GraphQL API Flow

This sequence diagram shows the complete flow of a GraphQL API request, including query parsing, resolution, and data fetching with DataLoader optimization.

```mermaid
sequenceDiagram
    participant Client
    participant Gateway as API Gateway
    participant GraphQL as GraphQL Server
    participant Schema as Schema Registry
    participant Resolver as Resolvers
    participant DataLoader
    participant Cache as Redis Cache
    participant DB as PostgreSQL
    participant BT as Bigtable

    Client->>Gateway: POST /graphql
    Note over Client: Query: devices {<br/>  id, name, status<br/>  sessions(limit: 10) {<br/>    id, start_time<br/>  }<br/>}

    Gateway->>Gateway: Validate JWT
    Gateway->>Gateway: Rate limiting check
    Gateway->>GraphQL: Forward request + user context

    GraphQL->>Schema: Parse & validate query
    Schema-->>GraphQL: AST + validation result

    alt Invalid query
        GraphQL-->>Client: 400 Bad Request<br/>(validation errors)
    end

    GraphQL->>GraphQL: Build execution plan
    GraphQL->>GraphQL: Apply query complexity limits

    GraphQL->>Resolver: Execute devices resolver

    Resolver->>Cache: Check cache for devices
    alt Cache hit
        Cache-->>Resolver: Cached devices data
    else Cache miss
        Resolver->>DataLoader: Load devices (batched)
        DataLoader->>DB: SELECT * FROM devices<br/>WHERE user_id = ?
        DB-->>DataLoader: Device records
        DataLoader-->>Resolver: Device objects
        Resolver->>Cache: Store in cache (TTL: 5min)
    end

    loop For each device
        Resolver->>Resolver: Resolve sessions field
        Resolver->>DataLoader: Load sessions (batched)

        Note over DataLoader: Batch multiple<br/>session requests

        DataLoader->>DB: SELECT * FROM sessions<br/>WHERE device_id IN (?)<br/>ORDER BY start_time DESC<br/>LIMIT ?
        DB-->>DataLoader: Session records

        DataLoader->>BT: Get session metadata
        BT-->>DataLoader: Time-series metadata

        DataLoader-->>Resolver: Session objects
    end

    Resolver-->>GraphQL: Resolved data tree

    GraphQL->>GraphQL: Apply field-level permissions
    GraphQL->>GraphQL: Format response

    GraphQL-->>Gateway: GraphQL response
    Gateway->>Gateway: Add response headers
    Gateway-->>Client: 200 OK + JSON response

    Note over Client: Response: {<br/>  "data": {<br/>    "devices": [{<br/>      "id": "...",<br/>      "name": "...",<br/>      "status": "ONLINE",<br/>      "sessions": [...]<br/>    }]<br/>  }<br/>}
```

<Tabs items={['API Features', 'Query Examples', 'Performance']}>
  <Tabs.Tab>
    **GraphQL API Features:**

    | Feature | Description | Implementation |
    |---------|-------------|----------------|
    | **Type Safety** | Strongly typed schema | Strawberry GraphQL |
    | **Query Batching** | N+1 query prevention | DataLoader pattern |
    | **Caching** | Multi-level caching | Redis + in-memory |
    | **Subscriptions** | Real-time updates | WebSocket + Pub/Sub |
    | **Pagination** | Cursor-based pagination | Relay specification |
    | **Rate Limiting** | Per-user/per-IP limits | Token bucket algorithm |
    | **Query Complexity** | Prevent expensive queries | Depth + cost analysis |
    | **Field Permissions** | Fine-grained access control | Decorator-based |
    | **Error Handling** | Structured error responses | GraphQL error spec |
    | **Introspection** | Schema discovery | Disabled in production |

    **Security Features:**
    - JWT authentication with refresh tokens
    - Field-level authorization
    - Query depth limiting (max: 10 levels)
    - Query complexity scoring (max: 1000 points)
    - Rate limiting (100 req/min per user)
    - SQL injection prevention via parameterized queries
  </Tabs.Tab>

  <Tabs.Tab>
    **Common Query Patterns:**

    ```graphql
    # Device listing with nested data
    query GetDevices($filter: DeviceFilter, $pagination: PaginationInput) {
      devices(filter: $filter, pagination: $pagination) {
        edges {
          node {
            id
            name
            type
            status
            lastSeen
            sessions(first: 5) {
              edges {
                node {
                  id
                  startTime
                  status
                  patient {
                    id
                    externalId
                  }
                }
              }
            }
          }
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }

    # Real-time subscriptions
    subscription StreamNeuralData($sessionId: ID!) {
      neuralDataStream(sessionId: $sessionId) {
        timestamp
        channel
        value
        sessionId
      }
    }

    # Mutations with nested returns
    mutation StartSession($input: StartSessionInput!) {
      startSession(input: $input) {
        session {
          id
          status
          device {
            id
            name
          }
        }
        errors {
          field
          message
        }
      }
    }
    ```

    **DataLoader Batching Example:**
    ```python
    # Without DataLoader: N+1 queries
    devices = get_devices()  # 1 query
    for device in devices:
        sessions = get_sessions(device.id)  # N queries

    # With DataLoader: 2 queries total
    devices = get_devices()  # 1 query
    session_loader.load_many([d.id for d in devices])  # 1 batched query
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Performance Metrics:**

    | Operation | Response Time | Throughput |
    |-----------|---------------|------------|
    | Simple query (cached) | Less than 10ms | 10,000 req/s |
    | Simple query (DB) | Less than 50ms | 2,000 req/s |
    | Complex nested query | Less than 200ms | 500 req/s |
    | Subscription setup | Less than 100ms | 1,000 conn/s |
    | Data streaming | Less than 5ms/msg | 100,000 msg/s |

    **Optimization Strategies:**

    ```python
    # Efficient resolver with DataLoader
    @strawberry.field
    async def sessions(
        self,
        info: Info,
        first: int = 10,
        after: Optional[str] = None
    ) -> SessionConnection:
        # Use DataLoader for batching
        loader = info.context.dataloaders.session_loader

        # Apply cursor-based pagination
        sessions = await loader.load(
            SessionQuery(
                device_id=self.id,
                limit=first + 1,  # Extra for hasNextPage
                after_cursor=after
            )
        )

        # Build connection response
        has_next = len(sessions) > first
        edges = [
            SessionEdge(
                node=session,
                cursor=encode_cursor(session.id)
            )
            for session in sessions[:first]
        ]

        return SessionConnection(
            edges=edges,
            page_info=PageInfo(
                has_next_page=has_next,
                end_cursor=edges[-1].cursor if edges else None
            )
        )
    ```

    **Caching Strategy:**
    - L1 Cache: In-memory LRU (100MB, 1min TTL)
    - L2 Cache: Redis (1GB, 5min TTL)
    - Cache key pattern: `gql:{query_hash}:{variables_hash}`
    - Cache invalidation: Event-driven via Pub/Sub
  </Tabs.Tab>
</Tabs>

### Real-time Data Streaming Flow

This flowchart shows the complete real-time data streaming architecture, from device data acquisition through WebSocket delivery to clients.

```mermaid
flowchart TB
    subgraph "Data Sources"
        D1[OpenBCI Device]
        D2[Emotiv Device]
        D3[Custom BCI]
        D4[Synthetic Device]
    end

    subgraph "Device Layer"
        DM[Device Manager]
        RB[Ring Buffer<br/>Per Device]
        DS[Data Sampler<br/>250-1000Hz]
        IM[Impedance Monitor]
    end

    subgraph "Processing Layer"
        AG[Aggregator<br/>Time Sync]
        FT[Filter<br/>Bandpass/Notch]
        WN[Windowing<br/>1-4s windows]
        FE[Feature Extraction<br/>Parallel]
    end

    subgraph "Transport Layer"
        PS[Pub/Sub Publisher]
        WS[WebSocket Server]
        SB[Stream Buffer<br/>Backpressure]
        CM[Connection Manager]
    end

    subgraph "Client Layer"
        WC1[WebSocket Client 1]
        WC2[WebSocket Client 2]
        WC3[WebSocket Client N]
        VZ[Visualizations]
    end

    subgraph "Storage Layer"
        BT[Bigtable<br/>Time-series]
        RD[Redis<br/>Hot Data]
        S3[Cloud Storage<br/>Raw Files]
    end

    D1 -->|USB/BLE| DM
    D2 -->|SDK| DM
    D3 -->|LSL| DM
    D4 -->|Internal| DM

    DM --> RB
    RB --> DS
    DS --> IM

    IM -->|Valid| AG
    IM -->|Invalid| DM

    AG --> FT
    FT --> WN
    WN --> FE

    FE --> PS
    FE --> WS
    FE --> BT

    PS -->|Topics| CM
    WS --> SB
    SB --> CM

    CM --> WC1
    CM --> WC2
    CM --> WC3

    WC1 --> VZ
    WC2 --> VZ
    WC3 --> VZ

    PS --> BT
    PS --> RD
    PS --> S3

    style D1 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D2 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D3 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D4 fill:#4285F4,stroke:#1a73e8,color:#fff
    style DM fill:#34A853,stroke:#188038,color:#fff
    style AG fill:#FBBC04,stroke:#F9AB00,color:#000
    style PS fill:#EA4335,stroke:#C5221F,color:#fff
    style WS fill:#EA4335,stroke:#C5221F,color:#fff
    style BT fill:#9E9E9E,stroke:#616161,color:#fff
```

<Tabs items={['Streaming Architecture', 'WebSocket Protocol', 'Implementation']}>
  <Tabs.Tab>
    **Streaming Components:**

    | Component | Purpose | Capacity | Latency |
    |-----------|---------|----------|---------|
    | **Ring Buffer** | Per-device circular buffer | 10s of data | Less than 1ms write |
    | **Data Sampler** | Downsample/upsample to target rate | 250-1000 Hz | Less than 1ms |
    | **Aggregator** | Time-align multi-device streams | 32 devices | Less than 5ms |
    | **Filter** | Remove noise and artifacts | Real-time | Less than 2ms |
    | **Windowing** | Create analysis windows | 1-4s windows | Less than 1ms |
    | **Feature Extraction** | Compute real-time features | 100+ features | Less than 10ms |
    | **WebSocket Server** | Real-time client delivery | 1000+ clients | Less than 5ms |
    | **Stream Buffer** | Handle client backpressure | 1MB per client | Variable |

    **Data Flow Rates:**
    - Raw data: 32 channels Ã 1000 Hz Ã 4 bytes = 128 KB/s per device
    - Feature data: 100 features Ã 10 Hz Ã 4 bytes = 4 KB/s per device
    - WebSocket messages: ~100-1000 msg/s per client
    - Pub/Sub throughput: 10,000+ msg/s total
  </Tabs.Tab>

  <Tabs.Tab>
    **WebSocket Message Protocol:**

    ```typescript
    // Client -> Server Messages
    interface ClientMessage {
      type: "subscribe" | "unsubscribe" | "configure";
      payload: SubscribePayload | UnsubscribePayload | ConfigPayload;
    }

    interface SubscribePayload {
      streamType: "raw" | "features" | "predictions";
      deviceIds: string[];
      channels?: number[];
      sampleRate?: number;
    }

    // Server -> Client Messages
    interface ServerMessage {
      type: "data" | "event" | "error" | "ack";
      timestamp: number;
      payload: DataPayload | EventPayload | ErrorPayload;
    }

    interface DataPayload {
      deviceId: string;
      streamType: "raw" | "features" | "predictions";
      data: Float32Array | FeatureData | PredictionData;
      sequenceNumber: number;
    }

    interface EventPayload {
      eventType: "device_connected" | "device_disconnected" | "impedance_change";
      deviceId: string;
      details: any;
    }
    ```

    **Connection Lifecycle:**
    1. Client connects: `ws://api/v1/streaming/neural-data`
    2. Server sends: `{"type": "ack", "payload": {"connectionId": "..."}}`
    3. Client subscribes: `{"type": "subscribe", "payload": {...}}`
    4. Server streams data: `{"type": "data", "payload": {...}}`
    5. Client can configure filtering, sampling rate, etc.
    6. Server handles backpressure with buffering
    7. Clean disconnect or timeout handling
  </Tabs.Tab>

  <Tabs.Tab>
    **Streaming Implementation:**

    ```python
    class StreamingService:
        def __init__(self):
            self.connections: Dict[str, WebSocketConnection] = {}
            self.device_streams: Dict[str, DeviceStream] = {}
            self.buffer_size = 10000  # samples per device

        async def handle_connection(self, websocket: WebSocket):
            connection_id = str(uuid4())
            connection = WebSocketConnection(
                id=connection_id,
                websocket=websocket,
                subscriptions=set(),
                buffer=StreamBuffer(max_size=1024*1024)  # 1MB
            )

            self.connections[connection_id] = connection

            try:
                # Send acknowledgment
                await websocket.send_json({
                    "type": "ack",
                    "timestamp": time.time(),
                    "payload": {"connectionId": connection_id}
                })

                # Handle messages
                while True:
                    message = await websocket.receive_json()
                    await self._handle_client_message(connection, message)

            except WebSocketDisconnect:
                await self._cleanup_connection(connection_id)

        async def stream_data(self, device_id: str, data: np.ndarray):
            # Process through pipeline
            filtered = await self.filter_data(data)
            windowed = self.window_data(filtered)
            features = await self.extract_features(windowed)

            # Publish to subscribers
            tasks = []
            for conn_id, connection in self.connections.items():
                if device_id in connection.subscriptions:
                    tasks.append(
                        self._send_to_client(connection, {
                            "type": "data",
                            "timestamp": time.time(),
                            "payload": {
                                "deviceId": device_id,
                                "streamType": "features",
                                "data": features.tolist(),
                                "sequenceNumber": self._get_sequence()
                            }
                        })
                    )

            # Send concurrently with backpressure handling
            await asyncio.gather(*tasks, return_exceptions=True)

        async def _send_to_client(self, connection: WebSocketConnection, message: dict):
            # Check buffer space
            if connection.buffer.available_space() < len(message):
                # Apply backpressure
                await connection.buffer.wait_for_space()

            # Queue message
            connection.buffer.put(message)

            # Send if not already sending
            if not connection.is_sending:
                await self._flush_buffer(connection)
    ```

    **Performance Optimizations:**
    - Zero-copy buffers for raw data
    - SIMD operations for filtering
    - Parallel feature extraction
    - Message batching for efficiency
    - Automatic quality degradation under load
  </Tabs.Tab>
</Tabs>

## Service Architecture

### Device Service

The Device Service manages all device-related operations and real-time data acquisition.

```mermaid
graph LR
    subgraph "Device Service Architecture"
        DR[Device Registry<br/>PostgreSQL]
        CP[Connection Pool<br/>Async Manager]
        SE[Streaming Engine<br/>Ring Buffers]
        HM[Health Monitor<br/>Prometheus]
        PS[Pub/Sub Client<br/>Publisher]
    end

    subgraph "Supported Devices"
        subgraph "Consumer BCIs"
            D1[OpenBCI<br/>Cyton/Ganglion]
            D2[Emotiv<br/>EPOC+/Insight]
            D3[Muse<br/>Muse 2/S]
            D4[NeuroSky<br/>MindWave]
        end

        subgraph "Research Systems"
            D5[g.tec<br/>g.USBamp]
            D6[BrainProducts<br/>actiCHamp]
            D7[ANT Neuro<br/>eegoâ¢]
            D8[BioSemi<br/>ActiveTwo]
        end

        subgraph "Clinical Arrays"
            D9[Blackrock<br/>Utah Array]
            D10[Plexon<br/>OmniPlex]
            D11[Custom LSL<br/>Lab Streaming]
        end
    end

    subgraph "Device Features"
        F1[Auto-Discovery<br/>mDNS/USB]
        F2[Retry Logic<br/>Exponential Backoff]
        F3[Impedance Check<br/>Real-time]
        F4[Signal Quality<br/>SNR Monitoring]
        F5[Multi-device Sync<br/>NTP/PTP]
        F6[Hot Swap<br/>Zero Downtime]
    end

    D1 --> DR
    D2 --> DR
    D3 --> DR
    D4 --> DR
    D5 --> DR
    D6 --> DR
    D7 --> DR
    D8 --> DR
    D9 --> DR
    D10 --> DR
    D11 --> DR

    DR --> CP
    CP --> SE
    SE --> HM
    SE --> PS
    PS -.-> |"Topics:<br/>device.connected<br/>device.data<br/>device.status"| PubSub[Cloud Pub/Sub]

    HM --> F1
    HM --> F2
    HM --> F3
    HM --> F4
    HM --> F5
    HM --> F6

    style DR fill:#ea4335,color:#ffffff
    style CP fill:#ea4335,color:#ffffff
    style SE fill:#ea4335,color:#ffffff
    style HM fill:#ea4335,color:#ffffff
    style PS fill:#ea4335,color:#ffffff
    style PubSub fill:#fbbc04,color:#000000
    style D1 fill:#e3f2fd,color:#0d47a1
    style D2 fill:#e3f2fd,color:#0d47a1
    style D3 fill:#e3f2fd,color:#0d47a1
    style D4 fill:#e3f2fd,color:#0d47a1
    style D5 fill:#e8f5e9,color:#1b5e20
    style D6 fill:#e8f5e9,color:#1b5e20
    style D7 fill:#e8f5e9,color:#1b5e20
    style D8 fill:#e8f5e9,color:#1b5e20
    style D9 fill:#fce4ec,color:#880e4f
    style D10 fill:#fce4ec,color:#880e4f
    style D11 fill:#fce4ec,color:#880e4f
```

**Technical Specifications:**
- Written in Python 3.12 with asyncio
- Uses lock-free ring buffers for data
- Implements backpressure mechanisms
- Sub-100ms latency guarantee

### Processing Service

The Processing Service handles all signal processing and feature extraction operations using GCP AI/ML services.

```mermaid
graph TB
    subgraph "Input Stream"
        PS1[Pub/Sub<br/>device.data]
        PS2[Stream Processing<br/>Dataflow]
    end

    subgraph "Real-Time Processing Pipeline"
        subgraph "Preprocessing (NumPy/SciPy)"
            PP1[Resampler<br/>Anti-aliasing]
            PP2[Filter Bank<br/>Butterworth/Chebyshev]
            PP3[Artifact Removal<br/>ICA/ASR]
            PP4[Windowing<br/>Sliding/Overlapping]
        end

        subgraph "Feature Extraction (MNE-Python)"
            FE1[Spectral Features<br/>FFT, PSD, Wavelets]
            FE2[Temporal Features<br/>Statistics, Entropy, Hjorth]
            FE3[Connectivity Metrics<br/>Coherence, PLV, PAC]
            FE4[Time-Frequency<br/>STFT, Morlet Wavelets]
        end

        subgraph "ML Pipeline (Vertex AI)"
            ML1[Feature Store<br/>Vector Database]
            ML2[AutoML Models<br/>Tabular Classification]
            ML3[Custom Models<br/>TensorFlow/PyTorch]
            ML4[Model Serving<br/>Vertex Endpoints]
        end

        subgraph "Classification Tasks"
            C1[Mental State<br/>Focus/Relaxation]
            C2[Sleep Staging<br/>NREM/REM/Wake]
            C3[Motor Imagery<br/>Left/Right Hand]
            C4[Seizure Prediction<br/>Pre-ictal Detection]
            C5[Cognitive Load<br/>Working Memory]
        end
    end

    subgraph "Output"
        OUT1[Pub/Sub<br/>processing.results]
        OUT2[Bigtable<br/>Feature Storage]
        OUT3[BigQuery<br/>Analytics]
    end

    PS1 --> PS2
    PS2 --> PP1

    PP1 --> PP2
    PP2 --> PP3
    PP3 --> PP4

    PP4 --> FE1
    PP4 --> FE2
    PP4 --> FE3
    PP4 --> FE4

    FE1 --> ML1
    FE2 --> ML1
    FE3 --> ML1
    FE4 --> ML1

    ML1 --> ML2
    ML1 --> ML3
    ML2 --> ML4
    ML3 --> ML4

    ML4 --> C1
    ML4 --> C2
    ML4 --> C3
    ML4 --> C4
    ML4 --> C5

    C1 --> OUT1
    C2 --> OUT1
    C3 --> OUT1
    C4 --> OUT1
    C5 --> OUT1

    FE1 --> OUT2
    FE2 --> OUT2
    FE3 --> OUT2
    FE4 --> OUT2

    OUT1 --> OUT3

    style PS1 fill:#fbbc04,color:#000000
    style PS2 fill:#fbbc04,color:#000000
    style PP1 fill:#4285f4,color:#ffffff
    style PP2 fill:#4285f4,color:#ffffff
    style PP3 fill:#4285f4,color:#ffffff
    style PP4 fill:#4285f4,color:#ffffff
    style FE1 fill:#34a853,color:#ffffff
    style FE2 fill:#34a853,color:#ffffff
    style FE3 fill:#34a853,color:#ffffff
    style FE4 fill:#34a853,color:#ffffff
    style ML1 fill:#ea4335,color:#ffffff
    style ML2 fill:#ea4335,color:#ffffff
    style ML3 fill:#ea4335,color:#ffffff
    style ML4 fill:#ea4335,color:#ffffff
    style C1 fill:#9c27b0,color:#ffffff
    style C2 fill:#9c27b0,color:#ffffff
    style C3 fill:#9c27b0,color:#ffffff
    style C4 fill:#9c27b0,color:#ffffff
    style C5 fill:#9c27b0,color:#ffffff
    style OUT1 fill:#fbbc04,color:#000000
    style OUT2 fill:#4285f4,color:#ffffff
    style OUT3 fill:#4285f4,color:#ffffff
```

### Data Service

The Data Service manages data persistence, retrieval, and analytics using GCP's multi-tier storage architecture.

```mermaid
graph LR
    subgraph "Data Ingestion"
        IN1[Pub/Sub<br/>Real-time Stream]
        IN2[Dataflow<br/>ETL Pipeline]
        IN3[Transfer Service<br/>Batch Import]
    end

    subgraph "Multi-Tier Storage Architecture"
        subgraph "Hot Tier (Real-time)"
            H1[Memorystore Redis<br/>Latest 5 min<br/>Sub-ms Access]
            H2[Bigtable<br/>Last 24 hours<br/>1-10ms Access]
        end

        subgraph "Warm Tier (Recent)"
            W1[Bigtable<br/>Last 30 days<br/>Column Families]
            W2[Cloud SQL<br/>Metadata & Sessions<br/>PostgreSQL HA]
        end

        subgraph "Cold Tier (Historical)"
            C1[Cloud Storage<br/>Nearline Class<br/>30+ days]
            C2[BigQuery<br/>Analytics Warehouse<br/>Partitioned Tables]
            C3[Archive Storage<br/>Coldline Class<br/>1+ year]
        end
    end

    subgraph "Data Access Patterns"
        AP1[Direct API<br/>Low Latency]
        AP2[Batch Export<br/>CSV/Parquet]
        AP3[BigQuery ML<br/>Analytics]
        AP4[Dataproc<br/>Spark/Hadoop]
    end

    IN1 --> H1
    IN1 --> H2
    IN2 --> W1
    IN3 --> C1

    H1 --> W1
    H2 --> W1
    W1 --> C1
    W2 --> C2
    C1 --> C2
    C2 --> C3

    H1 --> AP1
    H2 --> AP1
    W1 --> AP1
    C2 --> AP3
    C1 --> AP2
    C2 --> AP4

    style IN1 fill:#fbbc04,color:#000000
    style IN2 fill:#fbbc04,color:#000000
    style IN3 fill:#fbbc04,color:#000000
    style H1 fill:#ea4335,color:#ffffff
    style H2 fill:#ea4335,color:#ffffff
    style W1 fill:#4285f4,color:#ffffff
    style W2 fill:#4285f4,color:#ffffff
    style C1 fill:#34a853,color:#ffffff
    style C2 fill:#34a853,color:#ffffff
    style C3 fill:#34a853,color:#ffffff
    style AP1 fill:#9c27b0,color:#ffffff
    style AP2 fill:#9c27b0,color:#ffffff
    style AP3 fill:#9c27b0,color:#ffffff
    style AP4 fill:#9c27b0,color:#ffffff
```

## Data Flow & Latency

### Real-Time Data Pipeline

```mermaid
graph LR
    subgraph "Device Layer"
        D1[OpenBCI<br/>Cyton]
        D2[Emotiv<br/>EPOC+]
        D3[Clinical<br/>Arrays]
        D4[Custom<br/>LSL]
    end

    subgraph "Acquisition & Publishing"
        A1[Device Manager<br/>GKE Pod]
        A2[Protocol Handler<br/>Python Async]
        A3[Publisher Client<br/>Pub/Sub SDK]
    end

    subgraph "Cloud Pub/Sub Topics"
        T1[device.connected<br/>Control Plane]
        T2[device.data.raw<br/>250-1000 Hz Stream]
        T3[device.status<br/>Health Metrics]
        T4[device.impedance<br/>Quality Check]
    end

    subgraph "Stream Processing"
        SP1[Dataflow Job<br/>Apache Beam]
        SP2[Windowing<br/>1s Tumbling]
        SP3[Aggregation<br/>Statistics]
    end

    subgraph "Processing Topics"
        T5[processing.features<br/>Extracted Features]
        T6[processing.results<br/>Classifications]
        T7[processing.alerts<br/>Anomalies]
    end

    subgraph "Storage Sinks"
        S1[Bigtable<br/>Raw Data]
        S2[BigQuery<br/>Analytics]
        S3[Cloud Storage<br/>Archive]
        S4[Monitoring<br/>Metrics]
    end

    subgraph "API Layer"
        API1[Cloud Run<br/>REST API]
        API2[WebSocket<br/>Real-time]
        API3[gRPC<br/>Low Latency]
    end

    D1 --> A1
    D2 --> A1
    D3 --> A1
    D4 --> A1

    A1 --> A2
    A2 --> A3

    A3 --> T1
    A3 --> T2
    A3 --> T3
    A3 --> T4

    T2 --> SP1
    SP1 --> SP2
    SP2 --> SP3

    SP3 --> T5
    SP3 --> T6
    SP3 --> T7

    T2 --> S1
    T5 --> S2
    T6 --> S2
    T7 --> S4
    SP3 --> S3

    S1 --> API1
    S2 --> API2
    T6 --> API3

    style D1 fill:#e3f2fd,color:#0d47a1
    style D2 fill:#e3f2fd,color:#0d47a1
    style D3 fill:#e3f2fd,color:#0d47a1
    style D4 fill:#e3f2fd,color:#0d47a1
    style A1 fill:#ea4335,color:#ffffff
    style A2 fill:#ea4335,color:#ffffff
    style A3 fill:#ea4335,color:#ffffff
    style T1 fill:#fbbc04,color:#000000
    style T2 fill:#fbbc04,color:#000000
    style T3 fill:#fbbc04,color:#000000
    style T4 fill:#fbbc04,color:#000000
    style T5 fill:#fbbc04,color:#000000
    style T6 fill:#fbbc04,color:#000000
    style T7 fill:#fbbc04,color:#000000
    style SP1 fill:#34a853,color:#ffffff
    style SP2 fill:#34a853,color:#ffffff
    style SP3 fill:#34a853,color:#ffffff
    style S1 fill:#4285f4,color:#ffffff
    style S2 fill:#4285f4,color:#ffffff
    style S3 fill:#4285f4,color:#ffffff
    style S4 fill:#4285f4,color:#ffffff
    style API1 fill:#9c27b0,color:#ffffff
    style API2 fill:#9c27b0,color:#ffffff
    style API3 fill:#9c27b0,color:#ffffff
```

### Latency Budget

<Table>
  <thead>
    <Table.Tr>
      <Table.Th>Stage</Table.Th>
      <Table.Th>Budget</Table.Th>
      <Table.Th>Actual</Table.Th>
      <Table.Th>Notes</Table.Th>
    </Table.Tr>
  </thead>
  <tbody>
    <Table.Tr>
      <Table.Td>Device Acquisition</Table.Td>
      <Table.Td>20ms</Table.Td>
      <Table.Td>10-15ms</Table.Td>
      <Table.Td>Hardware dependent</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Network Transfer</Table.Td>
      <Table.Td>15ms</Table.Td>
      <Table.Td>5-10ms</Table.Td>
      <Table.Td>Optimized protocols</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Buffering</Table.Td>
      <Table.Td>5ms</Table.Td>
      <Table.Td>&lt;2ms</Table.Td>
      <Table.Td>Lock-free queues</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Feature Extraction</Table.Td>
      <Table.Td>15ms</Table.Td>
      <Table.Td>10-15ms</Table.Td>
      <Table.Td>SIMD optimized</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>ML Classification</Table.Td>
      <Table.Td>15ms</Table.Td>
      <Table.Td>5-10ms</Table.Td>
      <Table.Td>TensorRT/ONNX</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Processing</Table.Td>
      <Table.Td>10ms</Table.Td>
      <Table.Td>5-10ms</Table.Td>
      <Table.Td>Parallel pipelines</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Storage Write</Table.Td>
      <Table.Td>10ms</Table.Td>
      <Table.Td>5-8ms</Table.Td>
      <Table.Td>Async writes</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>API Response</Table.Td>
      <Table.Td>10ms</Table.Td>
      <Table.Td>5-8ms</Table.Td>
      <Table.Td>Cached responses</Table.Td>
    </Table.Tr>
  </tbody>
</Table>

## Scalability

### Horizontal Scaling Architecture

```mermaid
graph TB
    subgraph "External Traffic"
        ET[Cloud Load Balancer<br/>+ Cloud Armor]
    end

    subgraph "GKE Cluster - Regional"
        subgraph "Control Plane"
            CP[Kubernetes API<br/>HA Masters]
            HPA[Horizontal Pod<br/>Autoscaler]
            VPA[Vertical Pod<br/>Autoscaler]
            CA[Cluster<br/>Autoscaler]
        end

        subgraph "Node Pool 1 - General"
            subgraph "Zone A"
                N1A[n2-standard-4<br/>4 vCPU, 16GB RAM]
                P1A[Device Service<br/>2-10 replicas]
                P2A[API Service<br/>3-20 replicas]
            end

            subgraph "Zone B"
                N1B[n2-standard-4<br/>4 vCPU, 16GB RAM]
                P1B[Device Service<br/>2-10 replicas]
                P2B[API Service<br/>3-20 replicas]
            end

            subgraph "Zone C"
                N1C[n2-standard-4<br/>4 vCPU, 16GB RAM]
                P1C[Device Service<br/>2-10 replicas]
                P2C[API Service<br/>3-20 replicas]
            end
        end

        subgraph "Node Pool 2 - ML/GPU"
            subgraph "GPU Zone A"
                N2A[n1-highmem-4<br/>+ T4 GPU]
                P3A[ML Service<br/>1-5 replicas]
            end

            subgraph "GPU Zone B"
                N2B[n1-highmem-4<br/>+ T4 GPU]
                P3B[ML Service<br/>1-5 replicas]
            end
        end

        subgraph "Node Pool 3 - Memory"
            N3[n2-highmem-8<br/>8 vCPU, 64GB RAM]
            P4[Processing Service<br/>2-8 replicas]
        end
    end

    subgraph "Autoscaling Metrics"
        M1[CPU Usage<br/>>70%]
        M2[Memory Usage<br/>>80%]
        M3[Pub/Sub Queue<br/>>1000 msgs]
        M4[Custom Metrics<br/>Device Count]
    end

    subgraph "Managed Services"
        MS1[Cloud SQL<br/>Auto-failover]
        MS2[Bigtable<br/>Auto-scaling]
        MS3[Pub/Sub<br/>Unlimited Scale]
        MS4[Cloud Storage<br/>â Scale]
    end

    ET --> CP
    CP --> HPA
    CP --> VPA
    CP --> CA

    HPA --> P1A
    HPA --> P2A
    HPA --> P3A
    HPA --> P4

    CA --> N1A
    CA --> N1B
    CA --> N1C
    CA --> N2A
    CA --> N2B
    CA --> N3

    M1 --> HPA
    M2 --> HPA
    M3 --> HPA
    M4 --> HPA

    P1A --> MS1
    P2A --> MS2
    P3A --> MS3
    P4 --> MS4

    style ET fill:#34a853,color:#ffffff
    style CP fill:#ea4335,color:#ffffff
    style HPA fill:#fbbc04,color:#000000
    style VPA fill:#fbbc04,color:#000000
    style CA fill:#fbbc04,color:#000000
    style N1A fill:#4285f4,color:#ffffff
    style N1B fill:#4285f4,color:#ffffff
    style N1C fill:#4285f4,color:#ffffff
    style N2A fill:#9c27b0,color:#ffffff
    style N2B fill:#9c27b0,color:#ffffff
    style N3 fill:#4285f4,color:#ffffff
    style MS1 fill:#0f9d58,color:#ffffff
    style MS2 fill:#0f9d58,color:#ffffff
    style MS3 fill:#0f9d58,color:#ffffff
    style MS4 fill:#0f9d58,color:#ffffff
```

### Resource Allocation

<Tabs items={['Device Service', 'Processing Service', 'Data Service']}>
  <Tabs.Tab>
    **Device Service Resources:**
    - CPU: 2-8 cores
    - Memory: 4-16 GB
    - Network: 1-10 Gbps
    - Scaling: By device count
  </Tabs.Tab>

  <Tabs.Tab>
    **Processing Service Resources:**
    - CPU: 8-32 cores
    - Memory: 32-128 GB
    - GPU: Optional (CUDA)
    - Scaling: By channel count
  </Tabs.Tab>

  <Tabs.Tab>
    **Data Service Resources:**
    - CPU: 4-16 cores
    - Memory: 16-64 GB
    - Storage: NVMe SSD
    - Scaling: By write throughput
  </Tabs.Tab>
</Tabs>

## Security Architecture

### Defense in Depth

```mermaid
graph TB
    subgraph "Edge Security"
        ES1[Cloud Armor<br/>DDoS Protection]
        ES2[Cloud CDN<br/>Edge Caching]
        ES3[SSL/TLS Policy<br/>TLS 1.3 Only]
    end

    subgraph "Network Security"
        NS1[VPC Security<br/>Private Google Access]
        NS2[Cloud Firewall<br/>Ingress/Egress Rules]
        NS3[Private Service Connect<br/>Internal Traffic]
        NS4[Cloud NAT<br/>Egress Control]
    end

    subgraph "Identity & Access"
        IA1[Cloud IAM<br/>Service Accounts]
        IA2[Workload Identity<br/>GKE Integration]
        IA3[Binary Authorization<br/>Container Security]
        IA4[Cloud Identity<br/>SSO/MFA]
    end

    subgraph "Data Security"
        DS1[Cloud KMS<br/>Encryption Keys]
        DS2[Secret Manager<br/>API Keys/Creds]
        DS3[Data Loss Prevention<br/>PII Scanning]
        DS4[CMEK<br/>Customer Keys]
    end

    subgraph "Monitoring & Compliance"
        MC1[Cloud Audit Logs<br/>Immutable Trail]
        MC2[Security Command Center<br/>Threat Detection]
        MC3[Cloud Asset Inventory<br/>Resource Tracking]
        MC4[Neural Ledger<br/>HIPAA Audit Trail]
    end

    subgraph "Runtime Security"
        RS1[Container Analysis<br/>Vulnerability Scanning]
        RS2[GKE Security<br/>Pod Security Policy]
        RS3[Istio Service Mesh<br/>mTLS]
        RS4[Cloud Trace<br/>Request Tracking]
    end

    ES1 --> NS1
    ES2 --> NS1
    ES3 --> NS1

    NS1 --> IA1
    NS2 --> IA1
    NS3 --> IA1
    NS4 --> IA1

    IA1 --> DS1
    IA2 --> DS1
    IA3 --> DS1
    IA4 --> DS1

    DS1 --> MC1
    DS2 --> MC1
    DS3 --> MC1
    DS4 --> MC1

    MC1 --> RS1
    MC2 --> RS1
    MC3 --> RS1
    MC4 --> RS1

    style ES1 fill:#ea4335,color:#ffffff
    style ES2 fill:#ea4335,color:#ffffff
    style ES3 fill:#ea4335,color:#ffffff
    style NS1 fill:#fbbc04,color:#000000
    style NS2 fill:#fbbc04,color:#000000
    style NS3 fill:#fbbc04,color:#000000
    style NS4 fill:#fbbc04,color:#000000
    style IA1 fill:#4285f4,color:#ffffff
    style IA2 fill:#4285f4,color:#ffffff
    style IA3 fill:#4285f4,color:#ffffff
    style IA4 fill:#4285f4,color:#ffffff
    style DS1 fill:#34a853,color:#ffffff
    style DS2 fill:#34a853,color:#ffffff
    style DS3 fill:#34a853,color:#ffffff
    style DS4 fill:#34a853,color:#ffffff
    style MC1 fill:#9c27b0,color:#ffffff
    style MC2 fill:#9c27b0,color:#ffffff
    style MC3 fill:#9c27b0,color:#ffffff
    style MC4 fill:#9c27b0,color:#ffffff
    style RS1 fill:#00acc1,color:#ffffff
    style RS2 fill:#00acc1,color:#ffffff
    style RS3 fill:#00acc1,color:#ffffff
    style RS4 fill:#00acc1,color:#ffffff
```

### Compliance Features

<Tabs items={['HIPAA', 'GDPR']}>
  <Tabs.Tab>
    **HIPAA Compliance:**
    - End-to-end encryption (AES-256)
    - Audit logging with immutability
    - Access controls (RBAC + ABAC)
    - Data retention policies
    - Business Associate Agreements
  </Tabs.Tab>

  <Tabs.Tab>
    **GDPR Compliance:**
    - Consent management
    - Right to deletion
    - Data portability
    - Privacy by design
    - Data minimization
  </Tabs.Tab>
</Tabs>

## Technology Stack

<Table>
  <thead>
    <Table.Tr>
      <Table.Th>Component</Table.Th>
      <Table.Th>Technology</Table.Th>
      <Table.Th>Justification</Table.Th>
    </Table.Tr>
  </thead>
  <tbody>
    <Table.Tr>
      <Table.Td>Backend</Table.Td>
      <Table.Td>Python 3.12 + FastAPI</Table.Td>
      <Table.Td>Async performance, ecosystem</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Real-time</Table.Td>
      <Table.Td>WebSocket + gRPC</Table.Td>
      <Table.Td>Low latency, bidirectional</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Message Bus</Table.Td>
      <Table.Td>Kafka + Redis Pub/Sub</Table.Td>
      <Table.Td>Scalability, persistence</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Time Series</Table.Td>
      <Table.Td>TimescaleDB</Table.Td>
      <Table.Td>PostgreSQL compatibility</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Object Store</Table.Td>
      <Table.Td>S3/MinIO</Table.Td>
      <Table.Td>Standard API, scalability</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Search</Table.Td>
      <Table.Td>Elasticsearch</Table.Td>
      <Table.Td>Full-text, aggregations</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>ML Inference</Table.Td>
      <Table.Td>ONNX Runtime + TensorRT</Table.Td>
      <Table.Td>Optimized inference</Table.Td>
    </Table.Tr>
    <Table.Tr>
      <Table.Td>Monitoring</Table.Td>
      <Table.Td>Prometheus + Grafana</Table.Td>
      <Table.Td>Industry standard</Table.Td>
    </Table.Tr>
  </tbody>
</Table>

## Deployment Architecture

### Multi-Project GCP Setup

```mermaid
graph TB
    subgraph "GitHub Actions"
        GA[GitHub Actions<br/>CI/CD Pipeline]
        WIF[Workload Identity<br/>Federation]
    end

    subgraph "Development Environment"
        subgraph "development-neurascale"
            DEV_GKE[GKE Cluster<br/>Dev Workloads]
            DEV_DB[Cloud SQL<br/>Dev Database]
            DEV_BT[Bigtable<br/>Dev Instance]
            DEV_PS[Pub/Sub<br/>Dev Topics]
        end
    end

    subgraph "Staging Environment"
        subgraph "staging-neurascale"
            STG_GKE[GKE Cluster<br/>Staging Workloads]
            STG_DB[Cloud SQL<br/>Staging Database]
            STG_BT[Bigtable<br/>Staging Instance]
            STG_PS[Pub/Sub<br/>Staging Topics]
        end
    end

    subgraph "Production Environment"
        subgraph "production-neurascale"
            subgraph "us-central1"
                PROD_GKE_C[GKE Cluster<br/>Production Workloads<br/>n2-standard-16]
                PROD_DB_C[Cloud SQL<br/>Production Primary<br/>HA Configuration]
                PROD_BT_C[Bigtable<br/>Production Instance<br/>SSD Storage]
                PROD_PS_C[Pub/Sub<br/>Production Topics]
            end
            subgraph "us-east1"
                PROD_GKE_E[GKE Cluster<br/>Production Workloads<br/>n2-standard-16]
                PROD_DB_E[Cloud SQL<br/>Production Replica<br/>Read Replicas]
                PROD_BT_E[Bigtable<br/>Replication Instance]
                PROD_PS_E[Pub/Sub<br/>Multi-region Topics]
            end
        end
    end

    subgraph "Shared Services"
        MON[Cloud Monitoring<br/>Logging & Metrics]
        SEC[Security Command Center<br/>Threat Detection]
        REG[Artifact Registry<br/>Container Images]
        TF[Terraform State<br/>Cloud Storage]
    end

    GA --> WIF
    WIF --> |develop branch| DEV_GKE
    WIF --> |PR branches| STG_GKE
    WIF --> |main branch| PROD_GKE_C
    WIF --> |main branch| PROD_GKE_E

    DEV_GKE --> MON
    STG_GKE --> MON
    PROD_GKE_C --> MON
    PROD_GKE_E --> MON

    PROD_DB_C -.->|Replication| PROD_DB_E
    PROD_BT_C -.->|Replication| PROD_BT_E

    style GA fill:#4285f4,color:#fff
    style WIF fill:#4285f4,color:#fff
    style DEV_GKE fill:#34a853,color:#fff
    style STG_GKE fill:#fbbc04,color:#000
    style PROD_GKE_C fill:#ea4335,color:#fff
    style PROD_GKE_E fill:#ea4335,color:#fff
    style MON fill:#4285f4,color:#fff
    style SEC fill:#ea4335,color:#fff
    style REG fill:#34a853,color:#fff
    style TF fill:#fbbc04,color:#000
```

### Disaster Recovery

<Callout type="info">
  **RTO/RPO Targets:**
  - RTO (Recovery Time Objective): &lt;1 hour
  - RPO (Recovery Point Objective): &lt;5 minutes
</Callout>

**Backup Strategy:**
- Continuous replication to standby region
- Point-in-time recovery for 30 days
- Automated failover with health checks
- Regular DR drills

## Performance Optimization

### Optimization Techniques

<Tabs items={['Zero-Copy', 'SIMD', 'GPU', 'Async I/O']}>
  <Tabs.Tab>
    **Zero-Copy Data Transfer**
    ```python
    # Shared memory segments
    buffer = mmap.mmap(-1, size)
    # Direct memory access
    numpy_array = np.frombuffer(buffer)
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **SIMD Vectorization**
    ```python
    # NumPy with MKL backend
    # AVX2/AVX-512 instructions
    filtered = np.convolve(data, kernel, mode='same')
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **GPU Acceleration**
    ```python
    # CuPy for GPU processing
    import cupy as cp
    gpu_data = cp.asarray(cpu_data)
    gpu_fft = cp.fft.fft(gpu_data)
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Async I/O**
    ```python
    # AsyncIO for concurrent operations
    async def process_streams(devices):
        tasks = [process_device(d) for d in devices]
        await asyncio.gather(*tasks)
    ```
  </Tabs.Tab>
</Tabs>

## Future Enhancements

### Roadmap

```mermaid
timeline
    title Architecture Evolution Timeline

    section Completed
        Phase 1-8 : Core infrastructure, Real-time processing, Device integration
        Phase 9-12 : GCP migration, Vertex AI integration, Multi-region deployment
        Phase 13-16 : MCP Server, Neural CLI, Production hardening, Security implementation

    section Q1 2026
        Phase 17-18 : Edge deployment, Neural Ledger integration, Advanced ML pipelines

    section Q2 2026
        Phase 19-20 : Federated learning, Privacy-preserving ML, Clinical trials platform

    section Q3 2026
        Phase 21-22 : Neuromorphic computing, Hardware accelerators, Real-time collaboration

    section Q4 2026+
        Phase 23+ : Quantum-ready algorithms, Brain-computer standards, Global platform
```

### Research Areas

- **Neuromorphic Computing** - Brain-inspired hardware integration
- **Spiking Neural Networks** - Event-based processing
- **Reservoir Computing** - Efficient temporal processing
- **Brain-Computer Interface Standards** - Industry standardization

## Related Documentation

- [API Documentation](/api-documentation) - Complete API reference
- [Neural Management System](/neural-management-system) - Neural Engine details
- [Security](/security) - Security and compliance details
- [Contributing Guide](/contributing) - Development guidelines
