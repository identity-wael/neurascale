name: Neural Engine Deploy (Simplified)

on:
  push:
    branches: [main]
    paths:
      - 'neural-engine/**'
      - '.github/workflows/neural-engine-deploy-simplified.yml'
  pull_request:
    paths:
      - 'neural-engine/**'
      - '.github/workflows/neural-engine-deploy-simplified.yml'

env:
  PYTHON_VERSION: '3.12'
  GCP_REGION: 'northamerica-northeast1'
  TF_VERSION: '1.5.7'

jobs:
  # Determine target environment based on event
  setup:
    runs-on: ubuntu-latest
    outputs:
      environment: ${{ steps.determine.outputs.environment }}
      project_id: ${{ steps.determine.outputs.project_id }}
    steps:
      - id: determine
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "environment=staging" >> $GITHUB_OUTPUT
            echo "project_id=staging-neurascale" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "environment=production" >> $GITHUB_OUTPUT
            echo "project_id=production-neurascale" >> $GITHUB_OUTPUT
          else
            echo "environment=development" >> $GITHUB_OUTPUT
            echo "project_id=development-neurascale" >> $GITHUB_OUTPUT
          fi

  # Run tests
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('neural-engine/requirements*.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        working-directory: neural-engine
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt
          pip install -e .

      - name: Run linting
        working-directory: neural-engine
        run: |
          python -m black --check src/ tests/ examples/
          python -m flake8 src/ tests/ examples/ --config=.flake8

      - name: Run type checking
        working-directory: neural-engine
        run: |
          python -m mypy src/ --config-file=mypy.ini --namespace-packages

      - name: Run unit tests
        working-directory: neural-engine
        run: |
          pytest tests/ -v --cov=src --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        with:
          file: ./neural-engine/coverage.xml
          fail_ci_if_error: false

  # Build Docker images and Cloud Functions package
  build:
    needs: [setup, test]
    runs-on: ubuntu-latest
    environment: ${{ needs.setup.outputs.environment }}
    permissions:
      contents: read
      id-token: write
    strategy:
      matrix:
        service: [api, ingestion, processor]
    # Build Docker images only after test passes
    if: success()
    steps:
      - uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/555656387124/locations/global/workloadIdentityPools/github-actions/providers/github'
          service_account: 'github-actions@neurascale.iam.gserviceaccount.com'

      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        with:
          driver-opts: |
            image=moby/buildkit:latest
            network=host

      - name: Cache Docker layers
        uses: actions/cache@v4
        with:
          path: /tmp/.buildx-cache
          key: ${{ runner.os }}-buildx-${{ matrix.service }}-${{ github.sha }}
          restore-keys: |
            ${{ runner.os }}-buildx-${{ matrix.service }}-
            ${{ runner.os }}-buildx-

      - name: Configure Docker for Artifact Registry
        run: |
          gcloud auth configure-docker ${{ env.GCP_REGION }}-docker.pkg.dev

      - name: Check if Artifact Registry exists
        id: check_registry
        run: |
          if gcloud artifacts repositories describe neural-engine-${{ needs.setup.outputs.environment }} \
             --location=${{ env.GCP_REGION }} \
             --project=${{ needs.setup.outputs.project_id }} 2>/dev/null; then
            echo "exists=true" >> $GITHUB_OUTPUT
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "Artifact Registry not found. It will be created by Terraform."
          fi

      - name: Build and push Docker image
        if: steps.check_registry.outputs.exists == 'true'
        uses: docker/build-push-action@v5
        with:
          context: neural-engine
          file: neural-engine/docker/${{ matrix.service }}.Dockerfile
          push: true
          tags: |
            ${{ env.GCP_REGION }}-docker.pkg.dev/${{ needs.setup.outputs.project_id }}/neural-engine-${{ needs.setup.outputs.environment }}/${{ matrix.service }}:${{ github.sha }}
            ${{ env.GCP_REGION }}-docker.pkg.dev/${{ needs.setup.outputs.project_id }}/neural-engine-${{ needs.setup.outputs.environment }}/${{ matrix.service }}:latest
          cache-from: type=local,src=/tmp/.buildx-cache
          cache-to: type=local,dest=/tmp/.buildx-cache-new,mode=max
          platforms: linux/amd64

      - name: Move cache
        if: steps.check_registry.outputs.exists == 'true'
        run: |
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache

  # Package Cloud Functions
  package-functions:
    needs: [setup, test]
    runs-on: ubuntu-latest
    environment: ${{ needs.setup.outputs.environment }}
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Package Cloud Functions
        working-directory: neural-engine/functions/stream_ingestion
        run: |
          # Create a requirements.txt with only production dependencies
          echo "google-cloud-pubsub>=2.18.0" > requirements.txt
          echo "google-cloud-bigtable>=2.21.0" >> requirements.txt
          echo "google-cloud-logging>=3.8.0" >> requirements.txt
          echo "functions-framework>=3.5.0" >> requirements.txt

          # Create deployment package
          zip -r functions-${{ needs.setup.outputs.environment }}.zip main.py requirements.txt

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/555656387124/locations/global/workloadIdentityPools/github-actions/providers/github'
          service_account: 'github-actions@neurascale.iam.gserviceaccount.com'

      - name: Upload functions package
        working-directory: neural-engine/functions/stream_ingestion
        run: |
          # Keep the zip file as an artifact for now
          # Terraform will handle the actual deployment
          echo "Functions package created: functions-${{ needs.setup.outputs.environment }}.zip"
          ls -la functions-${{ needs.setup.outputs.environment }}.zip

      - name: Upload functions artifact
        uses: actions/upload-artifact@v4
        with:
          name: functions-${{ needs.setup.outputs.environment }}
          path: neural-engine/functions/stream_ingestion/functions-${{ needs.setup.outputs.environment }}.zip

  # Deploy infrastructure and functions together
  deploy:
    needs: [setup, test, package-functions]
    runs-on: ubuntu-latest
    environment: ${{ needs.setup.outputs.environment }}
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TF_VERSION }}

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/555656387124/locations/global/workloadIdentityPools/github-actions/providers/github'
          service_account: 'github-actions@neurascale.iam.gserviceaccount.com'

      - name: Terraform Init
        working-directory: neural-engine/terraform
        run: |
          terraform init \
            -backend-config="bucket=neurascale-terraform-state" \
            -backend-config="prefix=neural-engine/${{ needs.setup.outputs.environment }}"

      - name: Terraform Plan
        working-directory: neural-engine/terraform
        run: |
          terraform plan \
            -var-file="environments/${{ needs.setup.outputs.environment }}.tfvars" \
            -var="github_actions_service_account=github-actions@neurascale.iam.gserviceaccount.com" \
            -out=tfplan

      - name: Terraform Apply
        if: github.event_name == 'push' || (github.event_name == 'pull_request' && needs.setup.outputs.environment == 'staging')
        working-directory: neural-engine/terraform
        run: |
          terraform apply -auto-approve tfplan

      - name: Mark deployment in monitoring
        if: success() && (github.event_name == 'push' || (github.event_name == 'pull_request' && needs.setup.outputs.environment == 'staging'))
        run: |
          gcloud logging write deployments \
            "Deployment completed for ${{ github.sha }}" \
            --severity=NOTICE \
            --resource=global \
            --labels=environment=${{ needs.setup.outputs.environment }},version=${{ github.sha }},status=success

  # Verify deployment
  verify:
    needs: [setup, deploy]
    runs-on: ubuntu-latest
    environment: ${{ needs.setup.outputs.environment }}
    permissions:
      contents: read
      id-token: write
    steps:
      - uses: actions/checkout@v4

      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: 'projects/555656387124/locations/global/workloadIdentityPools/github-actions/providers/github'
          service_account: 'github-actions@neurascale.iam.gserviceaccount.com'

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install test dependencies
        run: |
          pip install google-cloud-pubsub google-cloud-bigtable

      - name: Verify infrastructure
        run: |
          # Verify Pub/Sub topics exist
          gcloud pubsub topics list --project=${{ needs.setup.outputs.project_id }} | grep "neural-data-eeg-${{ needs.setup.outputs.environment }}"

          # Note: Cloud Functions will be deployed separately
          echo "Skipping Cloud Functions verification - will deploy separately"

          # Verify Bigtable instance exists
          gcloud bigtable instances list --project=${{ needs.setup.outputs.project_id }} | grep "neural-data-${{ needs.setup.outputs.environment }}"

      - name: Run integration test
        run: |
          python -c "
          import json
          import time
          from google.cloud import pubsub_v1

          publisher = pubsub_v1.PublisherClient()
          topic_path = publisher.topic_path('${{ needs.setup.outputs.project_id }}', 'neural-data-eeg-${{ needs.setup.outputs.environment }}')

          test_message = {
              'device_id': 'test_device',
              'timestamp': time.time(),
              'data': [[0.0] * 256] * 8
          }

          future = publisher.publish(topic_path, json.dumps(test_message).encode())
          print(f'Published test message: {future.result()}')
          "
