import { Tabs, Callout, Table } from 'nextra/components'

# System Modeling

This section provides detailed system modeling diagrams that illustrate the internal workings of NeuraScale, including data flows, state machines, sequence diagrams, and entity relationships.

## Overview

The system modeling documentation uses industry-standard diagram types to represent:

1. **Flowcharts** - Step-by-step process flows for key operations
2. **Sequence Diagrams** - Time-ordered interactions between system components
3. **State Diagrams** - State transitions for devices, sessions, and processing pipelines
4. **Entity Relationship Diagrams** - Data model relationships and schemas
5. **Component Interaction Diagrams** - Detailed service communication patterns

## Component Interaction Diagram

This diagram shows the complete system architecture and how all services interact with each other, including synchronous and asynchronous communication patterns.

```mermaid
graph TB
    subgraph "Client Layer"
        WEB[Web Console]
        SDK[Python/JS SDK]
        CLI[Neural CLI]
        MCP[MCP Clients]
    end

    subgraph "API Gateway Layer"
        GW[API Gateway<br/>Load Balancer]
        AUTH[Auth Service<br/>JWT/OAuth2]
        RATE[Rate Limiter<br/>Token Bucket]
    end

    subgraph "Core Services"
        DEVICE[Device Service<br/>:8001]
        SESSION[Session Service<br/>:8002]
        PROCESS[Processing Service<br/>:8003]
        CLINICAL[Clinical Service<br/>:8004]
        ANALYSIS[Analysis Service<br/>:8005]
        ML[ML Service<br/>:8006]
    end

    subgraph "Real-time Layer"
        STREAM[Streaming Service<br/>WebSocket]
        PUBSUB[Pub/Sub Broker]
        QUEUE[Task Queue<br/>Celery]
    end

    subgraph "Data Layer"
        CACHE[Redis Cache<br/>Session/Results]
        PG[PostgreSQL<br/>Metadata]
        BT[Bigtable<br/>Time-series]
        BQ[BigQuery<br/>Analytics]
        GCS[Cloud Storage<br/>Raw Files]
    end

    subgraph "Infrastructure"
        MONITOR[Monitoring<br/>Prometheus]
        LOG[Logging<br/>Cloud Logging]
        TRACE[Tracing<br/>OpenTelemetry]
        SECRET[Secret Manager<br/>KMS]
    end

    %% Client connections
    WEB --> GW
    SDK --> GW
    CLI --> GW
    MCP --> CLINICAL
    MCP --> DEVICE
    MCP --> ANALYSIS

    %% Gateway routing
    GW --> AUTH
    AUTH --> RATE
    RATE --> DEVICE
    RATE --> SESSION
    RATE --> PROCESS
    RATE --> CLINICAL
    RATE --> ANALYSIS
    RATE --> ML

    %% Service interactions
    DEVICE -.->|Events| PUBSUB
    SESSION --> DEVICE
    SESSION --> PG
    PROCESS --> STREAM
    PROCESS -.->|Jobs| QUEUE
    CLINICAL --> SESSION
    ANALYSIS --> ML
    ML -.->|Inference| QUEUE

    %% Real-time connections
    STREAM --> CACHE
    PUBSUB --> STREAM
    PUBSUB --> BT
    QUEUE --> PROCESS
    QUEUE --> ML

    %% Data access patterns
    DEVICE --> CACHE
    SESSION --> PG
    PROCESS --> BT
    CLINICAL --> PG
    ANALYSIS --> BQ
    ML --> GCS

    %% Infrastructure connections
    DEVICE --> MONITOR
    SESSION --> MONITOR
    PROCESS --> MONITOR
    CLINICAL --> LOG
    ANALYSIS --> TRACE
    ML --> TRACE
    AUTH --> SECRET

    %% Legend
    classDef client fill:#4285F4,stroke:#1a73e8,color:#fff
    classDef gateway fill:#34A853,stroke:#188038,color:#fff
    classDef service fill:#FBBC04,stroke:#F9AB00,color:#000
    classDef realtime fill:#EA4335,stroke:#C5221F,color:#fff
    classDef data fill:#9E9E9E,stroke:#616161,color:#fff
    classDef infra fill:#673AB7,stroke:#512DA8,color:#fff

    class WEB,SDK,CLI,MCP client
    class GW,AUTH,RATE gateway
    class DEVICE,SESSION,PROCESS,CLINICAL,ANALYSIS,ML service
    class STREAM,PUBSUB,QUEUE realtime
    class CACHE,PG,BT,BQ,GCS data
    class MONITOR,LOG,TRACE,SECRET infra
```

<Tabs items={['Communication Patterns', 'Service Details', 'Integration Points']}>
  <Tabs.Tab>
    **Communication Types:**

    | Pattern | Use Case | Technology | Latency |
    |---------|----------|------------|---------|
    | **Synchronous REST** | CRUD operations | HTTP/2 + JSON | 10-100ms |
    | **GraphQL** | Complex queries | Apollo Federation | 20-200ms |
    | **WebSocket** | Real-time streaming | WS + MessagePack | Less than 5ms |
    | **Pub/Sub** | Event broadcasting | Cloud Pub/Sub | 10-50ms |
    | **gRPC** | Service-to-service | Protocol Buffers | 5-20ms |
    | **Task Queue** | Async processing | Celery + Redis | 100ms-5s |
    | **MCP** | AI assistant integration | JSON-RPC | 50-200ms |

    **Service Discovery:**
    - Kubernetes DNS for internal services
    - Cloud Endpoints for external APIs
    - Consul for dynamic service registry
    - Health checks via `/health` endpoints
  </Tabs.Tab>

  <Tabs.Tab>
    **Service Responsibilities:**

    ```yaml
    Device Service:
      - Device registration and management
      - Connection handling (USB, BLE, Network)
      - Impedance monitoring
      - Real-time data acquisition
      - Event publishing

    Session Service:
      - Session lifecycle management
      - Recording coordination
      - Metadata management
      - Access control
      - Audit logging

    Processing Service:
      - Signal filtering and preprocessing
      - Feature extraction
      - Window management
      - Stream processing
      - Quality metrics

    Clinical Service:
      - Patient management
      - Treatment protocols
      - Clinical workflows
      - HIPAA compliance
      - Report generation

    Analysis Service:
      - Batch analysis jobs
      - Statistical computations
      - Visualization generation
      - Export functionality
      - Result caching

    ML Service:
      - Model serving
      - Real-time inference
      - Model versioning
      - A/B testing
      - Performance monitoring
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Integration Points:**

    | Component | Inbound | Outbound | Protocol |
    |-----------|---------|----------|----------|
    | **API Gateway** | Clients | All services | HTTP/2 |
    | **Device Service** | Gateway, MCP | Pub/Sub, Cache | REST, gRPC |
    | **Session Service** | Gateway, Clinical | PostgreSQL, Device | REST, SQL |
    | **Processing Service** | Queue, Device | Bigtable, Stream | gRPC, WS |
    | **Clinical Service** | Gateway, MCP | PostgreSQL, Session | REST, SQL |
    | **Analysis Service** | Gateway, MCP | BigQuery, ML | REST, gRPC |
    | **ML Service** | Analysis, Queue | GCS, Cache | gRPC, REST |
    | **Streaming Service** | Processing | WebSocket clients | WS |
    | **Pub/Sub** | All services | Subscribers | Cloud Pub/Sub |
    | **Task Queue** | Services | Workers | AMQP |

    **Security Boundaries:**
    - Public internet → API Gateway (TLS 1.3)
    - API Gateway → Services (mTLS)
    - Services → Data stores (IAM + encryption)
    - Cross-service auth via service accounts
  </Tabs.Tab>
</Tabs>

## Process Flowcharts

### Device Connection Flow

This flowchart illustrates the complete process from device discovery to active data streaming, including error handling and recovery mechanisms.

```mermaid
flowchart TB
    Start([User Initiates Connection])

    Discovery{Device<br/>Discovery Mode}
    USB[USB Device<br/>Detection]
    BLE[BLE Device<br/>Scanning]
    WIFI[WiFi/LSL<br/>Network Scan]

    DeviceList[Display Available<br/>Devices List]

    SelectDevice{User Selects<br/>Device}

    CheckDriver{Driver<br/>Installed?}
    InstallDriver[Install Device<br/>Driver]

    InitConnection[Initialize<br/>Connection]

    Handshake{Protocol<br/>Handshake}

    Configure[Configure Device<br/>Parameters]
    SetSampleRate[Set Sample Rate<br/>250-1000 Hz]
    SetChannels[Select Active<br/>Channels]
    SetFilters[Configure<br/>Filters]

    Impedance{Impedance<br/>Check Required?}
    CheckImpedance[Run Impedance<br/>Check]
    ImpedanceOK{Impedance<br/><10kΩ?}

    StartBuffer[Initialize<br/>Ring Buffer]

    StartStream[Start Data<br/>Stream]

    Validate{Data<br/>Valid?}

    ProcessData[Process<br/>Incoming Data]
    PublishData[Publish to<br/>Pub/Sub]

    Streaming([Active Streaming])

    Error[Connection<br/>Error]
    Retry{Retry<br/>Connection?}

    Disconnect[Disconnect<br/>Device]

    End([Connection Ended])

    Start --> Discovery
    Discovery --> USB
    Discovery --> BLE
    Discovery --> WIFI

    USB --> DeviceList
    BLE --> DeviceList
    WIFI --> DeviceList

    DeviceList --> SelectDevice

    SelectDevice -->|No Device| End
    SelectDevice -->|Device Selected| CheckDriver

    CheckDriver -->|No| InstallDriver
    CheckDriver -->|Yes| InitConnection
    InstallDriver --> InitConnection

    InitConnection --> Handshake

    Handshake -->|Failed| Error
    Handshake -->|Success| Configure

    Configure --> SetSampleRate
    SetSampleRate --> SetChannels
    SetChannels --> SetFilters
    SetFilters --> Impedance

    Impedance -->|Yes| CheckImpedance
    Impedance -->|No| StartBuffer

    CheckImpedance --> ImpedanceOK
    ImpedanceOK -->|No| Error
    ImpedanceOK -->|Yes| StartBuffer

    StartBuffer --> StartStream
    StartStream --> Validate

    Validate -->|Invalid| Error
    Validate -->|Valid| ProcessData

    ProcessData --> PublishData
    PublishData --> Streaming

    Streaming -->|Data Flow| ProcessData
    Streaming -->|User Disconnect| Disconnect
    Streaming -->|Connection Lost| Error

    Error --> Retry
    Retry -->|Yes| InitConnection
    Retry -->|No| Disconnect

    Disconnect --> End

    style Start fill:#34a853,color:#fff
    style Streaming fill:#4285f4,color:#fff
    style Error fill:#ea4335,color:#fff
    style End fill:#9c27b0,color:#fff
    style ProcessData fill:#fbbc04,color:#000
    style PublishData fill:#fbbc04,color:#000
```

<Tabs items={['Process Steps', 'Error Handling', 'Implementation Details']}>
  <Tabs.Tab>
    **Key Process Steps:**

    1. **Discovery Phase**
       - USB devices detected via serial port enumeration
       - BLE devices found through active scanning
       - WiFi/LSL devices discovered via mDNS broadcast

    2. **Connection Setup**
       - Driver verification and installation if needed
       - Protocol-specific handshake (varies by device type)
       - Connection parameters negotiation

    3. **Configuration**
       - Sample rate selection (250Hz, 500Hz, 1000Hz)
       - Channel mapping and activation
       - Digital filter configuration (notch, bandpass)

    4. **Quality Assurance**
       - Optional impedance checking per channel
       - Signal quality validation
       - Automatic bad channel detection

    5. **Data Streaming**
       - Zero-copy ring buffer initialization
       - Real-time data validation
       - Asynchronous Pub/Sub publishing
  </Tabs.Tab>

  <Tabs.Tab>
    **Error Handling Mechanisms:**

    - **Connection Failures**: Exponential backoff retry strategy
    - **Driver Issues**: Automatic driver download and installation
    - **Protocol Errors**: Fallback to compatible protocol versions
    - **Data Validation**: Packet checksum and timestamp verification
    - **Network Issues**: Automatic reconnection with state preservation
    - **Buffer Overflow**: Backpressure handling and flow control

    **Recovery Strategies:**
    ```python
    # Retry configuration
    MAX_RETRIES = 3
    INITIAL_BACKOFF = 1.0  # seconds
    MAX_BACKOFF = 30.0     # seconds
    BACKOFF_MULTIPLIER = 2.0
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Implementation References:**

    - **Device Discovery**: `neural-engine/src/devices/discovery_service.py`
    - **Connection Manager**: `neural-engine/src/devices/device_manager.py`
    - **Protocol Handlers**: `neural-engine/src/devices/protocols/`
    - **Ring Buffer**: `neural-engine/src/core/ring_buffer.py`
    - **Pub/Sub Client**: `neural-engine/src/messaging/pubsub_client.py`

    **Key Classes:**
    ```python
    class DeviceDiscoveryService:
        async def discover_devices(self) -> List[DeviceInfo]

    class DeviceManager:
        async def connect(self, device_id: str) -> Device
        async def configure(self, device: Device, config: DeviceConfig)
        async def start_streaming(self, device: Device)
    ```
  </Tabs.Tab>
</Tabs>

### Session Management Flow

This flowchart shows the complete lifecycle of a neural recording session from creation to completion.

```mermaid
flowchart TB
    Start([User Creates Session])

    CreateSession[Create Session<br/>Metadata]
    AssignID[Generate<br/>Session UUID]

    SelectDevices{Select<br/>Devices}
    ValidateDevices[Validate Device<br/>Availability]

    ConfigSession[Configure Session<br/>Parameters]
    SetDuration[Set Recording<br/>Duration]
    SetTriggers[Configure<br/>Event Triggers]
    SetStorage[Select Storage<br/>Options]

    InitStorage[Initialize<br/>Storage Backend]
    CreateFiles[Create Data<br/>Files]

    StartRecording[Start<br/>Recording]

    Recording([Active Recording])

    MonitorHealth{Monitor<br/>Session Health}

    PauseOption{User Action}
    Pause[Pause<br/>Recording]
    Resume[Resume<br/>Recording]

    StopRecording[Stop<br/>Recording]

    FinalizeData[Finalize<br/>Data Files]
    GenerateMetadata[Generate Session<br/>Metadata]
    ArchiveSession[Archive<br/>Session]

    End([Session Complete])

    Error[Session<br/>Error]
    Cleanup[Cleanup<br/>Resources]

    Start --> CreateSession
    CreateSession --> AssignID
    AssignID --> SelectDevices

    SelectDevices --> ValidateDevices
    ValidateDevices --> ConfigSession

    ConfigSession --> SetDuration
    SetDuration --> SetTriggers
    SetTriggers --> SetStorage
    SetStorage --> InitStorage

    InitStorage --> CreateFiles
    CreateFiles --> StartRecording
    StartRecording --> Recording

    Recording --> MonitorHealth
    MonitorHealth -->|Healthy| PauseOption
    MonitorHealth -->|Error| Error

    PauseOption -->|Pause| Pause
    PauseOption -->|Stop| StopRecording
    PauseOption -->|Continue| Recording

    Pause --> Resume
    Resume --> Recording

    StopRecording --> FinalizeData
    FinalizeData --> GenerateMetadata
    GenerateMetadata --> ArchiveSession
    ArchiveSession --> End

    Error --> Cleanup
    Cleanup --> End

    style Start fill:#34a853,color:#fff
    style Recording fill:#4285f4,color:#fff
    style Error fill:#ea4335,color:#fff
    style End fill:#9c27b0,color:#fff
```

## Sequence Diagrams

### Device Streaming Sequence

This sequence diagram shows the time-ordered interactions between components during real-time device streaming.

```mermaid
sequenceDiagram
    participant Client
    participant API as API Gateway
    participant Auth as Auth Service
    participant DM as Device Manager
    participant Device as Device (OpenBCI)
    participant RB as Ring Buffer
    participant PS as Pub/Sub
    participant DS as Data Service
    participant WS as WebSocket

    Client->>API: POST /devices/{id}/stream/start
    API->>Auth: Validate JWT Token
    Auth-->>API: Token Valid + User Permissions
    API->>DM: StartStreaming(deviceId, config)

    DM->>Device: Connect()
    Device-->>DM: Connection Established

    DM->>Device: Configure(sampleRate, channels)
    Device-->>DM: Configuration ACK

    DM->>RB: Initialize(size=1MB)
    RB-->>DM: Buffer Ready

    DM->>Device: StartDataStream()
    Device-->>DM: Stream Started

    loop Every 4ms (250Hz)
        Device->>DM: DataPacket(samples, timestamp)
        DM->>DM: Validate Packet
        DM->>RB: Write(data)

        alt Buffer Threshold Reached (1000 samples)
            RB->>PS: PublishBatch(topic="device.data.raw")
            PS-->>DS: Store(data)
            PS-->>WS: Broadcast(data)
            WS-->>Client: Real-time Data
        end
    end

    Client->>API: POST /devices/{id}/stream/stop
    API->>DM: StopStreaming(deviceId)
    DM->>Device: StopDataStream()
    Device-->>DM: Stream Stopped
    DM->>RB: Flush()
    RB->>PS: PublishRemaining()
    DM->>Device: Disconnect()
    Device-->>DM: Disconnected
    DM-->>API: Success
    API-->>Client: 200 OK
```

<Tabs items={['Timing Details', 'Message Formats', 'Error Scenarios']}>
  <Tabs.Tab>
    **Timing Specifications:**

    | Operation | Typical Latency | Max Latency | Notes |
    |-----------|----------------|-------------|--------|
    | JWT Validation | 5-10ms | 50ms | Cached tokens faster |
    | Device Connection | 100-500ms | 2000ms | USB fastest, BLE slowest |
    | Configuration | 20-50ms | 100ms | Depends on parameters |
    | Buffer Init | Less than 1ms | 5ms | Pre-allocated memory |
    | Data Packet | 4ms | 8ms | 250Hz sampling rate |
    | Pub/Sub Publish | 5-10ms | 20ms | Async operation |
    | WebSocket Broadcast | 2-5ms | 10ms | Direct connection |

    **Buffering Strategy:**
    - Ring buffer size: 1MB (holds ~10s of data)
    - Batch threshold: 1000 samples (~4 seconds)
    - Flush interval: 100ms (failsafe)
  </Tabs.Tab>

  <Tabs.Tab>
    **Key Message Formats:**

    ```python
    # Data Packet Structure
    class NeuralDataPacket:
        timestamp: datetime       # UTC timestamp
        device_id: str           # Unique device identifier
        sequence_number: int     # Packet sequence counter
        samples: np.ndarray      # Shape: (n_channels, n_samples)
        sample_rate: int         # Hz (250, 500, 1000)
        channel_mask: int        # Active channels bitmask
        battery_level: float     # 0.0 to 1.0
        signal_quality: List[float]  # Per-channel quality
    ```

    **Pub/Sub Message Format:**
    ```json
    {
        "topic": "device.data.raw",
        "device_id": "openbci_cyton_001",
        "timestamp": "2024-01-15T10:30:45.123Z",
        "data": {
            "packets": ["..."],
            "batch_size": 1000,
            "duration_ms": 4000
        }
    }
    ```

    **WebSocket Frame Format:**
    ```json
    {
        "type": "data",
        "device_id": "openbci_cyton_001",
        "channels": [1.23, 4.56, 7.89],
        "timestamp": 1705317045123,
        "quality": [0.98, 0.99, 0.97]
    }
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Error Handling Sequences:**

    ```mermaid
    sequenceDiagram
        participant Client
        participant DM as Device Manager
        participant Device
        participant PS as Pub/Sub

        Note over Client,PS: Scenario 1: Connection Lost
        Device--xDM: Connection Lost
        DM->>DM: Detect Disconnection
        DM->>PS: Publish("device.disconnected")
        DM->>DM: Attempt Reconnection

        alt Reconnection Successful
            DM->>Device: Connect()
            Device-->>DM: Connected
            DM->>Device: Resume Stream
            DM->>PS: Publish("device.reconnected")
        else Reconnection Failed
            DM->>PS: Publish("device.error")
            DM-->>Client: Error Notification
        end

        Note over Client,PS: Scenario 2: Buffer Overflow
        Device->>DM: DataPacket
        DM->>DM: Buffer Full Check
        DM->>PS: EmergencyFlush()
        DM->>DM: Apply Backpressure
        DM-->>Device: Slow Down Signal
    ```
  </Tabs.Tab>
</Tabs>

## State Diagrams

### Device State Machine

This state diagram shows all possible device states and the transitions between them, including guards and actions.

```mermaid
stateDiagram-v2
    [*] --> Disconnected

    Disconnected --> Connecting: connect()

    Connecting --> Connected: handshake_success
    Connecting --> Error: handshake_failed
    Connecting --> Disconnected: timeout

    Connected --> Configuring: configure()

    Configuring --> Ready: config_applied
    Configuring --> Error: config_failed

    Ready --> Streaming: start_stream()
    Ready --> Disconnected: disconnect()

    Streaming --> Paused: pause()
    Streaming --> Ready: stop_stream()
    Streaming --> Error: stream_error

    Paused --> Streaming: resume()
    Paused --> Ready: stop_stream()

    Error --> Reconnecting: auto_retry [retry_count < 3]
    Error --> Disconnected: manual_reset
    Error --> Failed: auto_retry [retry_count >= 3]

    Reconnecting --> Connected: reconnect_success
    Reconnecting --> Error: reconnect_failed

    Failed --> Disconnected: reset()

    Connected --> Disconnected: connection_lost
    Ready --> Disconnected: connection_lost
    Streaming --> Disconnected: connection_lost
    Paused --> Disconnected: connection_lost

    state Connected {
        [*] --> Idle
        Idle --> CheckingImpedance: check_impedance()
        CheckingImpedance --> Idle: complete
    }

    state Streaming {
        [*] --> Active
        Active --> Buffering: buffer_full
        Buffering --> Active: buffer_flushed
    }
```

<Tabs items={['State Descriptions', 'Transition Events', 'Implementation']}>
  <Tabs.Tab>
    **Device States:**

    | State | Description | Allowed Actions |
    |-------|-------------|-----------------|
    | **Disconnected** | No active connection to device | Connect |
    | **Connecting** | Establishing connection | Cancel |
    | **Connected** | Connected but not configured | Configure, Disconnect, Check Impedance |
    | **Configuring** | Applying device settings | Cancel |
    | **Ready** | Configured and ready to stream | Start Stream, Disconnect |
    | **Streaming** | Actively receiving data | Pause, Stop, Monitor |
    | **Paused** | Streaming suspended | Resume, Stop |
    | **Error** | Recoverable error state | Retry, Reset |
    | **Reconnecting** | Attempting automatic recovery | Cancel |
    | **Failed** | Unrecoverable error | Manual Reset |

    **Composite States:**
    - **Connected**: Contains sub-states for impedance checking
    - **Streaming**: Contains sub-states for buffer management
  </Tabs.Tab>

  <Tabs.Tab>
    **State Transitions:**

    ```python
    # Transition Guards
    class TransitionGuards:
        @staticmethod
        def can_retry(device: Device) -> bool:
            return device.retry_count < MAX_RETRIES

        @staticmethod
        def has_valid_config(device: Device) -> bool:
            return device.validate_configuration()

        @staticmethod
        def buffer_available(device: Device) -> bool:
            return device.buffer.free_space > MIN_BUFFER_SIZE

    # Transition Actions
    class TransitionActions:
        @staticmethod
        async def on_connect(device: Device):
            device.retry_count = 0
            await device.initialize_driver()

        @staticmethod
        async def on_streaming_start(device: Device):
            await device.buffer.initialize()
            await device.start_data_thread()

        @staticmethod
        async def on_error(device: Device, error: Exception):
            await device.log_error(error)
            await device.notify_error_handlers(error)
            device.retry_count += 1
    ```

    **Event Triggers:**
    - User actions: `connect()`, `disconnect()`, `start_stream()`, `stop_stream()`
    - System events: `connection_lost`, `buffer_full`, `data_timeout`
    - Error events: `handshake_failed`, `config_failed`, `stream_error`
  </Tabs.Tab>

  <Tabs.Tab>
    **State Machine Implementation:**

    ```python
    from enum import Enum, auto
    from typing import Optional, Callable

    class DeviceState(Enum):
        DISCONNECTED = auto()
        CONNECTING = auto()
        CONNECTED = auto()
        CONFIGURING = auto()
        READY = auto()
        STREAMING = auto()
        PAUSED = auto()
        ERROR = auto()
        RECONNECTING = auto()
        FAILED = auto()

    class DeviceStateMachine:
        def __init__(self, device_id: str):
            self.device_id = device_id
            self.state = DeviceState.DISCONNECTED
            self.retry_count = 0
            self.state_handlers = {}
            self.transition_callbacks = []

        def register_state_handler(
            self,
            state: DeviceState,
            handler: Callable
        ):
            self.state_handlers[state] = handler

        async def transition_to(
            self,
            new_state: DeviceState,
            event: Optional[str] = None
        ):
            old_state = self.state

            # Validate transition
            if not self._is_valid_transition(old_state, new_state):
                raise InvalidTransitionError(
                    f"Cannot transition from {old_state} to {new_state}"
                )

            # Execute exit actions
            await self._execute_exit_actions(old_state)

            # Update state
            self.state = new_state

            # Execute entry actions
            await self._execute_entry_actions(new_state)

            # Notify callbacks
            for callback in self.transition_callbacks:
                await callback(old_state, new_state, event)
    ```

    **Usage Example:**
    ```python
    # neural-engine/src/devices/state_machine.py
    device_sm = DeviceStateMachine("openbci_001")
    device_sm.register_state_handler(
        DeviceState.STREAMING,
        handle_streaming_state
    )

    await device_sm.transition_to(DeviceState.CONNECTING)
    ```
  </Tabs.Tab>
</Tabs>

## Entity Relationship Diagrams

### Core Data Model

This ER diagram shows the relationships between core entities in the NeuraScale system.

```mermaid
erDiagram
    User ||--o{ Session : creates
    User ||--o{ Device : owns
    User {
        string user_id PK
        string email UK
        string name
        string role
        datetime created_at
        datetime last_login
        boolean is_active
    }

    Device ||--o{ Session : participates_in
    Device ||--o{ DeviceStatus : has
    Device {
        string device_id PK
        string user_id FK
        string device_type
        string serial_number UK
        string firmware_version
        json configuration
        datetime registered_at
        datetime last_seen
    }

    Session ||--o{ Recording : contains
    Session ||--o{ Event : logs
    Session {
        string session_id PK
        string user_id FK
        string name
        datetime start_time
        datetime end_time
        string status
        json metadata
        string storage_path
    }

    Recording ||--o{ DataChunk : stores
    Recording ||--o{ Feature : extracts
    Recording {
        string recording_id PK
        string session_id FK
        string device_id FK
        datetime timestamp
        int sample_rate
        int channel_count
        string data_format
        bigint sample_count
    }

    DataChunk ||--o{ Sample : contains
    DataChunk {
        string chunk_id PK
        string recording_id FK
        int sequence_number
        datetime start_time
        datetime end_time
        blob compressed_data
        string storage_location
        int size_bytes
    }

    Sample {
        bigint sample_id PK
        string chunk_id FK
        int channel
        float value
        int sample_index
        float quality_score
    }

    Feature ||--o{ Classification : produces
    Feature {
        string feature_id PK
        string recording_id FK
        string feature_type
        json feature_vector
        datetime computed_at
        string algorithm_version
    }

    Classification {
        string classification_id PK
        string feature_id FK
        string model_id FK
        string class_label
        float confidence
        json probabilities
        datetime predicted_at
    }

    Event {
        string event_id PK
        string session_id FK
        string event_type
        json event_data
        datetime timestamp
        string severity
    }

    DeviceStatus {
        string status_id PK
        string device_id FK
        float battery_level
        json impedance_values
        float signal_quality
        datetime measured_at
        boolean is_streaming
    }
```

<Tabs items={['Entity Details', 'Relationships', 'Database Schema']}>
  <Tabs.Tab>
    **Core Entities:**

    | Entity | Description | Primary Storage |
    |--------|-------------|-----------------|
    | **User** | System users (researchers, clinicians) | PostgreSQL |
    | **Device** | Neural recording devices | PostgreSQL |
    | **Session** | Recording sessions | PostgreSQL |
    | **Recording** | Continuous data from a device | PostgreSQL + Bigtable |
    | **DataChunk** | Compressed time-series segments | Bigtable |
    | **Sample** | Individual data points | Bigtable |
    | **Feature** | Extracted features from recordings | BigQuery |
    | **Classification** | ML model predictions | BigQuery |
    | **Event** | Session events and markers | PostgreSQL |
    | **DeviceStatus** | Real-time device metrics | Redis + PostgreSQL |

    **Data Volume Estimates:**
    - Users: ~1,000
    - Devices: ~10,000
    - Sessions: ~100,000/month
    - Recordings: ~1M/month
    - Samples: ~100B/month (at 250Hz)
    - Features: ~10M/month
    - Classifications: ~10M/month
  </Tabs.Tab>

  <Tabs.Tab>
    **Relationship Cardinalities:**

    ```sql
    -- One-to-Many Relationships
    User (1) ----< (N) Session
    User (1) ----< (N) Device
    Device (1) ----< (N) Session
    Device (1) ----< (N) DeviceStatus
    Session (1) ----< (N) Recording
    Session (1) ----< (N) Event
    Recording (1) ----< (N) DataChunk
    Recording (1) ----< (N) Feature
    DataChunk (1) ----< (N) Sample
    Feature (1) ----< (N) Classification

    -- Many-to-Many Relationships (via junction tables)
    Device <----> Session (via Recording)
    User <----> Device (ownership can be transferred)
    ```

    **Referential Integrity Rules:**
    - CASCADE DELETE: Session → Recording → DataChunk → Sample
    - RESTRICT DELETE: User with active Sessions
    - SET NULL: Device deletion sets Recording.device_id to NULL
    - NO ACTION: Feature deletion prevents Classification deletion
  </Tabs.Tab>

  <Tabs.Tab>
    **Physical Database Schema:**

    ```sql
    -- PostgreSQL Tables
    CREATE TABLE users (
        user_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        email VARCHAR(255) UNIQUE NOT NULL,
        name VARCHAR(255) NOT NULL,
        role VARCHAR(50) NOT NULL CHECK (role IN ('admin', 'researcher', 'clinician')),
        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_login TIMESTAMP WITH TIME ZONE,
        is_active BOOLEAN DEFAULT true
    );

    CREATE TABLE devices (
        device_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
        user_id UUID REFERENCES users(user_id),
        device_type VARCHAR(50) NOT NULL,
        serial_number VARCHAR(255) UNIQUE NOT NULL,
        firmware_version VARCHAR(50),
        configuration JSONB DEFAULT '{}',
        registered_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
        last_seen TIMESTAMP WITH TIME ZONE,
        INDEX idx_user_devices (user_id),
        INDEX idx_device_type (device_type)
    );

    -- Bigtable Schema
    Table: neural_data
    Row Key: {recording_id}#{timestamp}#{channel}
    Column Families:
      - raw: raw_value, quality_score
      - meta: sample_rate, device_id
      - features: fft, psd, entropy

    -- BigQuery Schema
    Dataset: neurascale_analytics
    Tables:
      - features: Partitioned by DATE(computed_at)
      - classifications: Partitioned by DATE(predicted_at)
      - aggregated_metrics: Materialized views for dashboards
    ```
  </Tabs.Tab>
</Tabs>

### Authentication Sequence

This sequence diagram shows the complete authentication and authorization flow using JWT tokens.

```mermaid
sequenceDiagram
    participant Client
    participant API as API Gateway
    participant Auth as Auth Service
    participant DB as PostgreSQL
    participant Redis
    participant KMS as Cloud KMS

    Note over Client,KMS: Initial Login Flow

    Client->>API: POST /auth/login {email, password}
    API->>Auth: ValidateCredentials(email, password)
    Auth->>DB: SELECT user WHERE email = ?
    DB-->>Auth: User record

    Auth->>Auth: VerifyPassword(hash, password)

    alt Invalid Credentials
        Auth-->>API: 401 Unauthorized
        API-->>Client: 401 {error: "Invalid credentials"}
    else Valid Credentials
        Auth->>KMS: GenerateTokenPair()
        KMS->>KMS: Sign JWT with private key
        KMS-->>Auth: {access_token, refresh_token}

        Auth->>Redis: SETEX session:{user_id} TTL=3600
        Auth->>DB: UPDATE user SET last_login = NOW()

        Auth-->>API: TokenPair + User info
        API-->>Client: 200 {tokens, user}
    end

    Note over Client,KMS: Authenticated Request Flow

    Client->>API: GET /devices {Authorization: Bearer <token>}
    API->>Auth: ValidateToken(access_token)

    Auth->>Redis: GET session:{user_id}

    alt Session Not Found
        Auth->>KMS: VerifySignature(token)

        alt Invalid Token
            Auth-->>API: 401 Unauthorized
            API-->>Client: 401 {error: "Invalid token"}
        else Valid Token
            Auth->>Auth: CheckExpiration(token)

            alt Token Expired
                Auth-->>API: 401 {error: "Token expired"}
                API-->>Client: 401 {error: "Token expired"}
            else Token Valid
                Auth->>Redis: SETEX session:{user_id} TTL=3600
                Auth-->>API: User context
                API->>API: Process request
                API-->>Client: 200 {data}
            end
        end
    else Session Found
        Auth-->>API: User context (from cache)
        API->>API: Process request
        API-->>Client: 200 {data}
    end

    Note over Client,KMS: Token Refresh Flow

    Client->>API: POST /auth/refresh {refresh_token}
    API->>Auth: RefreshTokens(refresh_token)

    Auth->>KMS: VerifySignature(refresh_token)
    Auth->>DB: SELECT user WHERE id = ? AND is_active = true

    alt Valid Refresh Token
        Auth->>KMS: GenerateTokenPair()
        KMS-->>Auth: {new_access_token, new_refresh_token}
        Auth->>Redis: SETEX session:{user_id} TTL=3600
        Auth-->>API: New TokenPair
        API-->>Client: 200 {tokens}
    else Invalid Refresh Token
        Auth->>Redis: DEL session:{user_id}
        Auth-->>API: 401 Unauthorized
        API-->>Client: 401 {error: "Invalid refresh token"}
    end
```

<Tabs items={['Token Details', 'Security Features', 'Implementation']}>
  <Tabs.Tab>
    **JWT Token Structure:**

    ```json
    {
      "header": {
        "alg": "RS256",
        "typ": "JWT",
        "kid": "key_id_2024"
      },
      "payload": {
        "sub": "user_uuid",
        "email": "user@example.com",
        "role": "researcher",
        "permissions": ["read:devices", "write:sessions"],
        "iat": 1705320000,
        "exp": 1705323600,
        "iss": "neurascale.io",
        "aud": "neurascale-api"
      },
      "signature": "..."
    }
    ```

    **Token Lifetimes:**
    - Access Token: 1 hour
    - Refresh Token: 30 days
    - Session Cache: 1 hour (sliding window)

    **Token Storage:**
    - Client: Secure storage (HttpOnly cookies or secure local storage)
    - Server: Redis for session caching
    - Keys: Cloud KMS for signing keys
  </Tabs.Tab>

  <Tabs.Tab>
    **Security Measures:**

    1. **Password Security**
       - Argon2id hashing (memory-hard)
       - Salt per password
       - Configurable work factors

    2. **Token Security**
       - RS256 asymmetric signing
       - Key rotation every 90 days
       - Token binding to IP/device

    3. **Rate Limiting**
       - Login: 5 attempts per 15 minutes
       - Token refresh: 10 per hour
       - API calls: 1000 per hour

    4. **Additional Protection**
       - CSRF tokens for web clients
       - Secure headers (HSTS, CSP)
       - Request signing for sensitive operations

    ```python
    # Rate limiting implementation
    @rate_limit(
        key=lambda: f"login:{request.remote_addr}",
        rate="5/15m",
        method="sliding_window"
    )
    async def login(credentials: LoginRequest):
        # Login logic
        pass
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Authentication Service Implementation:**

    ```python
    from datetime import datetime, timedelta
    from typing import Optional, Tuple
    import jwt
    from argon2 import PasswordHasher
    from redis import Redis

    class AuthService:
        def __init__(self, kms_client, db, redis: Redis):
            self.kms = kms_client
            self.db = db
            self.redis = redis
            self.ph = PasswordHasher()

        async def authenticate(
            self,
            email: str,
            password: str
        ) -> Tuple[str, str]:
            # Fetch user
            user = await self.db.fetch_one(
                "SELECT * FROM users WHERE email = ?",
                email
            )

            if not user:
                raise AuthenticationError("Invalid credentials")

            # Verify password
            try:
                self.ph.verify(user.password_hash, password)
            except:
                raise AuthenticationError("Invalid credentials")

            # Generate tokens
            access_token = await self._generate_access_token(user)
            refresh_token = await self._generate_refresh_token(user)

            # Cache session
            await self._cache_session(user.id, {
                "user_id": user.id,
                "email": user.email,
                "role": user.role,
                "permissions": user.permissions
            })

            return access_token, refresh_token

        async def _generate_access_token(self, user) -> str:
            payload = {
                "sub": str(user.id),
                "email": user.email,
                "role": user.role,
                "permissions": user.permissions,
                "iat": datetime.utcnow(),
                "exp": datetime.utcnow() + timedelta(hours=1)
            }

            # Sign with KMS
            return await self.kms.sign_jwt(payload)
    ```
  </Tabs.Tab>
</Tabs>

### Clinical Workflow Sequence

This sequence diagram shows the complete clinical workflow from patient registration through session recording and report generation, including all HIPAA compliance checkpoints.

```mermaid
sequenceDiagram
    participant Clinician
    participant Portal as Clinical Portal
    participant Clinical as Clinical Service
    participant Patient as Patient Service
    participant Session as Session Service
    participant Device as Device Service
    participant Storage as Storage Layer
    participant Audit as Audit Trail
    participant Report as Report Service

    Note over Clinician,Report: Patient Registration & Consent

    Clinician->>Portal: Register new patient
    Portal->>Clinical: POST /patients {demographics}
    Clinical->>Audit: Log access attempt
    Clinical->>Patient: CreatePatient(data)

    Patient->>Patient: Generate patient ID
    Patient->>Patient: Encrypt PII data
    Patient->>Storage: Store encrypted patient record
    Patient->>Audit: Log patient creation
    Patient-->>Clinical: Patient record created

    Clinical->>Clinical: Generate consent forms
    Clinical-->>Portal: Consent documents
    Portal-->>Clinician: Display consent forms

    Clinician->>Portal: Upload signed consent
    Portal->>Clinical: POST /patients/{id}/consent
    Clinical->>Storage: Store consent document
    Clinical->>Audit: Log consent received

    Note over Clinician,Report: Session Scheduling & Preparation

    Clinician->>Portal: Schedule session
    Portal->>Clinical: POST /sessions/schedule
    Clinical->>Session: CreateSession(patient_id, datetime)
    Session->>Session: Validate time slot
    Session->>Device: ReserveDevice(type, datetime)
    Device-->>Session: Device reserved
    Session->>Storage: Store session metadata
    Session->>Audit: Log session scheduled
    Session-->>Clinical: Session created
    Clinical-->>Portal: Session details

    Note over Clinician,Report: Session Recording

    Clinician->>Portal: Start session
    Portal->>Clinical: POST /sessions/{id}/start
    Clinical->>Clinical: Verify consent status
    Clinical->>Session: StartRecording(session_id)

    Session->>Device: ConnectDevice(device_id)
    Device->>Device: Impedance check
    Device-->>Session: Device ready

    Session->>Session: Initialize recording
    Session->>Audit: Log recording start

    loop Data Acquisition
        Device->>Device: Acquire neural data
        Device->>Session: Stream data packets
        Session->>Session: Validate & timestamp
        Session->>Storage: Write to Bigtable
        Session->>Audit: Log data access
    end

    Clinician->>Portal: Add clinical annotation
    Portal->>Clinical: POST /sessions/{id}/annotations
    Clinical->>Session: AddAnnotation(timestamp, note)
    Session->>Storage: Store annotation
    Session->>Audit: Log annotation added

    Clinician->>Portal: Stop session
    Portal->>Clinical: POST /sessions/{id}/stop
    Clinical->>Session: StopRecording()
    Session->>Device: DisconnectDevice()
    Session->>Session: Finalize recording
    Session->>Storage: Update session status
    Session->>Audit: Log recording complete
    Session-->>Clinical: Recording stopped

    Note over Clinician,Report: Analysis & Reporting

    Clinician->>Portal: Request analysis
    Portal->>Clinical: POST /sessions/{id}/analyze
    Clinical->>Clinical: Check permissions
    Clinical->>Report: GenerateReport(session_id)

    Report->>Storage: Fetch session data
    Report->>Report: Run analysis pipeline
    Report->>Report: Generate visualizations
    Report->>Report: Apply de-identification
    Report->>Storage: Store report
    Report->>Audit: Log report generation
    Report-->>Clinical: Report ready

    Clinical-->>Portal: Report URL
    Portal-->>Clinician: Display report

    Clinician->>Portal: Export for EHR
    Portal->>Clinical: GET /reports/{id}/export?format=hl7
    Clinical->>Report: ExportHL7(report_id)
    Report->>Report: Format to HL7 FHIR
    Report->>Audit: Log export request
    Report-->>Clinical: HL7 document
    Clinical-->>Portal: Download HL7
    Portal-->>Clinician: HL7 file
```

<Tabs items={['Workflow Steps', 'Compliance Checkpoints', 'Data Security']}>
  <Tabs.Tab>
    **Clinical Workflow Steps:**

    | Step | Duration | Requirements | Output |
    |------|----------|--------------|--------|
    | **Patient Registration** | 5-10 min | Demographics, Medical ID | Patient record |
    | **Consent Process** | 10-15 min | Signed forms, Witness | Legal consent |
    | **Device Setup** | 5-10 min | Impedance < 10kΩ | Ready device |
    | **Baseline Recording** | 2-5 min | Stable signal | Baseline data |
    | **Active Recording** | 20-60 min | Clinical protocol | Neural data |
    | **Data Review** | 5-10 min | Quality check | Validated data |
    | **Report Generation** | 2-5 min | Analysis complete | Clinical report |
    | **EHR Integration** | 1-2 min | HL7/FHIR format | Exported data |

    **Session Types:**
    - Diagnostic assessment
    - Treatment monitoring
    - Research protocol
    - Neurofeedback training
    - Pre/post intervention
  </Tabs.Tab>

  <Tabs.Tab>
    **HIPAA Compliance Points:**

    ```python
    class HIPAACompliance:
        def __init__(self):
            self.requirements = {
                "access_control": {
                    "authentication": "Multi-factor required",
                    "authorization": "Role-based (RBAC)",
                    "audit_trail": "All access logged"
                },
                "encryption": {
                    "at_rest": "AES-256-GCM",
                    "in_transit": "TLS 1.3",
                    "key_management": "Cloud KMS"
                },
                "data_integrity": {
                    "checksums": "SHA-256 for all files",
                    "versioning": "Immutable audit log",
                    "backup": "Daily encrypted backups"
                },
                "consent_management": {
                    "storage": "Encrypted document store",
                    "tracking": "Consent status per data use",
                    "revocation": "Immediate data isolation"
                }
            }

        async def validate_access(self, user, resource, action):
            # Check user authentication
            if not await self.verify_mfa(user):
                raise AuthenticationError("MFA required")

            # Check authorization
            if not self.rbac.has_permission(user, resource, action):
                raise AuthorizationError("Access denied")

            # Log access attempt
            await self.audit.log({
                "user": user.id,
                "resource": resource,
                "action": action,
                "timestamp": datetime.utcnow(),
                "ip_address": user.ip_address
            })
    ```

    **Audit Requirements:**
    - All data access logged
    - User actions traceable
    - Immutable audit trail
    - 7-year retention
    - Regular audit reviews
  </Tabs.Tab>

  <Tabs.Tab>
    **Data Security Measures:**

    | Layer | Protection | Implementation |
    |-------|------------|----------------|
    | **Application** | Input validation | OWASP guidelines |
    | **API** | Rate limiting | 100 req/min per user |
    | **Network** | Zero-trust architecture | Service mesh + mTLS |
    | **Storage** | Encryption at rest | AES-256-GCM |
    | **Database** | Column encryption | Sensitive fields only |
    | **Backup** | Encrypted archives | Daily + geographic redundancy |
    | **Key Management** | HSM-backed | Cloud KMS rotation |

    **Privacy Controls:**
    ```yaml
    de_identification:
      - Remove direct identifiers
      - Generalize quasi-identifiers
      - Add statistical noise
      - K-anonymity (k=5)

    data_minimization:
      - Collect only necessary data
      - Automatic data expiration
      - Purpose limitation
      - Consent-based retention

    access_patterns:
      clinician:
        - Own patients only
        - Time-limited access
        - Purpose declaration
      researcher:
        - De-identified data only
        - IRB approval required
        - Aggregated results
      admin:
        - Audit access only
        - No patient data view
        - System metrics only
    ```
  </Tabs.Tab>
</Tabs>

### Processing Pipeline State Machine

This state diagram shows the states and transitions for the real-time neural data processing pipeline.

```mermaid
stateDiagram-v2
    [*] --> Idle

    Idle --> Initializing: start_pipeline()

    Initializing --> Ready: init_complete
    Initializing --> Failed: init_error

    Ready --> Processing: data_available
    Ready --> Idle: stop_pipeline()

    Processing --> Buffering: buffer_threshold
    Processing --> FeatureExtraction: window_complete
    Processing --> Error: processing_error

    Buffering --> Processing: buffer_flushed
    Buffering --> Error: buffer_overflow

    FeatureExtraction --> Classification: features_ready
    FeatureExtraction --> Error: extraction_error

    Classification --> Publishing: results_ready
    Classification --> Error: classification_error

    Publishing --> Processing: publish_complete
    Publishing --> Error: publish_error

    Error --> Recovery: auto_recover [retry_count < 3]
    Error --> Failed: max_retries_exceeded
    Error --> Ready: manual_reset

    Recovery --> Processing: recovery_success
    Recovery --> Error: recovery_failed

    Failed --> Idle: reset_pipeline()

    Processing --> Ready: pause()
    Ready --> Processing: resume()

    state Processing {
        [*] --> Receiving
        Receiving --> Validating: packet_received
        Validating --> Filtering: valid_data
        Validating --> Dropping: invalid_data
        Filtering --> Windowing: filtered
        Windowing --> [*]: window_ready
        Dropping --> Receiving: continue
    }

    state FeatureExtraction {
        [*] --> Spectral
        Spectral --> Temporal: fft_complete
        Temporal --> Connectivity: stats_complete
        Connectivity --> [*]: features_complete
    }

    state Classification {
        [*] --> Loading
        Loading --> Inferencing: model_ready
        Inferencing --> PostProcessing: raw_predictions
        PostProcessing --> [*]: final_results
    }
```

<Tabs items={['Pipeline States', 'Processing Details', 'Implementation']}>
  <Tabs.Tab>
    **Pipeline States:**

    | State | Description | Processing Rate |
    |-------|-------------|-----------------|
    | **Idle** | No active processing | 0 samples/s |
    | **Initializing** | Loading models, allocating buffers | N/A |
    | **Ready** | Waiting for data | 0 samples/s |
    | **Processing** | Active data processing | 250-1000 samples/s |
    | **Buffering** | Accumulating data for batch | Variable |
    | **FeatureExtraction** | Computing features | ~10 windows/s |
    | **Classification** | ML inference | ~10 predictions/s |
    | **Publishing** | Sending results | Async |
    | **Error** | Recoverable error state | 0 samples/s |
    | **Recovery** | Attempting auto-recovery | N/A |
    | **Failed** | Unrecoverable error | 0 samples/s |

    **Composite State Details:**
    - **Processing**: Receiving → Validating → Filtering → Windowing
    - **FeatureExtraction**: Spectral → Temporal → Connectivity analysis
    - **Classification**: Model loading → Inference → Post-processing
  </Tabs.Tab>

  <Tabs.Tab>
    **Processing Pipeline Stages:**

    ```python
    # Pipeline Configuration
    PIPELINE_CONFIG = {
        "buffer_size": 10000,        # samples
        "window_size": 1000,         # samples (4s at 250Hz)
        "window_overlap": 0.5,       # 50% overlap
        "batch_size": 10,            # windows per batch
        "feature_dims": 128,         # feature vector size
        "model_timeout": 100,        # ms
        "publish_timeout": 50,       # ms
    }

    # Processing Stages
    class ProcessingStages:
        RECEIVING = "receiving"
        VALIDATING = "validating"
        FILTERING = "filtering"
        WINDOWING = "windowing"
        SPECTRAL = "spectral_features"
        TEMPORAL = "temporal_features"
        CONNECTIVITY = "connectivity_features"
        INFERENCE = "ml_inference"
        PUBLISHING = "result_publishing"
    ```

    **Performance Metrics:**
    - Latency per stage: 1-5ms
    - Total pipeline latency: Less than 50ms
    - Throughput: 1000 samples/s per channel
    - Feature extraction: 10ms per window
    - ML inference: 5-10ms per batch
  </Tabs.Tab>

  <Tabs.Tab>
    **Pipeline State Machine Implementation:**

    ```python
    from enum import Enum, auto
    from dataclasses import dataclass
    from typing import Optional, List
    import asyncio

    class PipelineState(Enum):
        IDLE = auto()
        INITIALIZING = auto()
        READY = auto()
        PROCESSING = auto()
        BUFFERING = auto()
        FEATURE_EXTRACTION = auto()
        CLASSIFICATION = auto()
        PUBLISHING = auto()
        ERROR = auto()
        RECOVERY = auto()
        FAILED = auto()

    @dataclass
    class PipelineContext:
        buffer: RingBuffer
        feature_extractor: FeatureExtractor
        classifier: MLClassifier
        publisher: ResultPublisher
        error_count: int = 0
        current_window: Optional[np.ndarray] = None
        features: Optional[np.ndarray] = None
        predictions: Optional[dict] = None

    class ProcessingPipeline:
        def __init__(self, config: dict):
            self.config = config
            self.state = PipelineState.IDLE
            self.context = None
            self._state_handlers = self._setup_handlers()

        async def start(self):
            await self.transition_to(PipelineState.INITIALIZING)

        async def process_data(self, data: np.ndarray):
            if self.state != PipelineState.READY:
                raise InvalidStateError(
                    f"Cannot process data in state {self.state}"
                )

            self.context.buffer.write(data)

            if self.context.buffer.ready_for_window():
                await self.transition_to(PipelineState.PROCESSING)

        async def transition_to(
            self,
            new_state: PipelineState
        ):
            # Execute state transition
            old_state = self.state
            self.state = new_state

            # Call appropriate handler
            handler = self._state_handlers.get(new_state)
            if handler:
                try:
                    await handler()
                except Exception as e:
                    await self._handle_error(e)

        async def _handle_processing(self):
            # Get window from buffer
            window = self.context.buffer.get_window(
                size=self.config["window_size"],
                overlap=self.config["window_overlap"]
            )

            # Validate and filter
            if not self._validate_window(window):
                return await self.transition_to(PipelineState.READY)

            filtered = await self._filter_window(window)
            self.context.current_window = filtered

            # Move to feature extraction
            await self.transition_to(PipelineState.FEATURE_EXTRACTION)
    ```
  </Tabs.Tab>
</Tabs>

### GraphQL API Flow

This sequence diagram shows the complete flow of a GraphQL API request, including query parsing, resolution, and data fetching with DataLoader optimization.

```mermaid
sequenceDiagram
    participant Client
    participant Gateway as API Gateway
    participant GraphQL as GraphQL Server
    participant Schema as Schema Registry
    participant Resolver as Resolvers
    participant DataLoader
    participant Cache as Redis Cache
    participant DB as PostgreSQL
    participant BT as Bigtable

    Client->>Gateway: POST /graphql
    Note over Client: Query: devices {<br/>  id, name, status<br/>  sessions(limit: 10) {<br/>    id, start_time<br/>  }<br/>}

    Gateway->>Gateway: Validate JWT
    Gateway->>Gateway: Rate limiting check
    Gateway->>GraphQL: Forward request + user context

    GraphQL->>Schema: Parse & validate query
    Schema-->>GraphQL: AST + validation result

    alt Invalid query
        GraphQL-->>Client: 400 Bad Request<br/>(validation errors)
    end

    GraphQL->>GraphQL: Build execution plan
    GraphQL->>GraphQL: Apply query complexity limits

    GraphQL->>Resolver: Execute devices resolver

    Resolver->>Cache: Check cache for devices
    alt Cache hit
        Cache-->>Resolver: Cached devices data
    else Cache miss
        Resolver->>DataLoader: Load devices (batched)
        DataLoader->>DB: SELECT * FROM devices<br/>WHERE user_id = ?
        DB-->>DataLoader: Device records
        DataLoader-->>Resolver: Device objects
        Resolver->>Cache: Store in cache (TTL: 5min)
    end

    loop For each device
        Resolver->>Resolver: Resolve sessions field
        Resolver->>DataLoader: Load sessions (batched)

        Note over DataLoader: Batch multiple<br/>session requests

        DataLoader->>DB: SELECT * FROM sessions<br/>WHERE device_id IN (?)<br/>ORDER BY start_time DESC<br/>LIMIT ?
        DB-->>DataLoader: Session records

        DataLoader->>BT: Get session metadata
        BT-->>DataLoader: Time-series metadata

        DataLoader-->>Resolver: Session objects
    end

    Resolver-->>GraphQL: Resolved data tree

    GraphQL->>GraphQL: Apply field-level permissions
    GraphQL->>GraphQL: Format response

    GraphQL-->>Gateway: GraphQL response
    Gateway->>Gateway: Add response headers
    Gateway-->>Client: 200 OK + JSON response

    Note over Client: Response: {<br/>  "data": {<br/>    "devices": [{<br/>      "id": "...",<br/>      "name": "...",<br/>      "status": "ONLINE",<br/>      "sessions": [...]<br/>    }]<br/>  }<br/>}
```

<Tabs items={['API Features', 'Query Examples', 'Performance']}>
  <Tabs.Tab>
    **GraphQL API Features:**

    | Feature | Description | Implementation |
    |---------|-------------|----------------|
    | **Type Safety** | Strongly typed schema | Strawberry GraphQL |
    | **Query Batching** | N+1 query prevention | DataLoader pattern |
    | **Caching** | Multi-level caching | Redis + in-memory |
    | **Subscriptions** | Real-time updates | WebSocket + Pub/Sub |
    | **Pagination** | Cursor-based pagination | Relay specification |
    | **Rate Limiting** | Per-user/per-IP limits | Token bucket algorithm |
    | **Query Complexity** | Prevent expensive queries | Depth + cost analysis |
    | **Field Permissions** | Fine-grained access control | Decorator-based |
    | **Error Handling** | Structured error responses | GraphQL error spec |
    | **Introspection** | Schema discovery | Disabled in production |

    **Security Features:**
    - JWT authentication with refresh tokens
    - Field-level authorization
    - Query depth limiting (max: 10 levels)
    - Query complexity scoring (max: 1000 points)
    - Rate limiting (100 req/min per user)
    - SQL injection prevention via parameterized queries
  </Tabs.Tab>

  <Tabs.Tab>
    **Common Query Patterns:**

    ```graphql
    # Device listing with nested data
    query GetDevices($filter: DeviceFilter, $pagination: PaginationInput) {
      devices(filter: $filter, pagination: $pagination) {
        edges {
          node {
            id
            name
            type
            status
            lastSeen
            sessions(first: 5) {
              edges {
                node {
                  id
                  startTime
                  status
                  patient {
                    id
                    externalId
                  }
                }
              }
            }
          }
        }
        pageInfo {
          hasNextPage
          endCursor
        }
      }
    }

    # Real-time subscriptions
    subscription StreamNeuralData($sessionId: ID!) {
      neuralDataStream(sessionId: $sessionId) {
        timestamp
        channel
        value
        sessionId
      }
    }

    # Mutations with nested returns
    mutation StartSession($input: StartSessionInput!) {
      startSession(input: $input) {
        session {
          id
          status
          device {
            id
            name
          }
        }
        errors {
          field
          message
        }
      }
    }
    ```

    **DataLoader Batching Example:**
    ```python
    # Without DataLoader: N+1 queries
    devices = get_devices()  # 1 query
    for device in devices:
        sessions = get_sessions(device.id)  # N queries

    # With DataLoader: 2 queries total
    devices = get_devices()  # 1 query
    session_loader.load_many([d.id for d in devices])  # 1 batched query
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Performance Metrics:**

    | Operation | Response Time | Throughput |
    |-----------|---------------|------------|
    | Simple query (cached) | Less than 10ms | 10,000 req/s |
    | Simple query (DB) | Less than 50ms | 2,000 req/s |
    | Complex nested query | Less than 200ms | 500 req/s |
    | Subscription setup | Less than 100ms | 1,000 conn/s |
    | Data streaming | Less than 5ms/msg | 100,000 msg/s |

    **Optimization Strategies:**

    ```python
    # Efficient resolver with DataLoader
    @strawberry.field
    async def sessions(
        self,
        info: Info,
        first: int = 10,
        after: Optional[str] = None
    ) -> SessionConnection:
        # Use DataLoader for batching
        loader = info.context.dataloaders.session_loader

        # Apply cursor-based pagination
        sessions = await loader.load(
            SessionQuery(
                device_id=self.id,
                limit=first + 1,  # Extra for hasNextPage
                after_cursor=after
            )
        )

        # Build connection response
        has_next = len(sessions) > first
        edges = [
            SessionEdge(
                node=session,
                cursor=encode_cursor(session.id)
            )
            for session in sessions[:first]
        ]

        return SessionConnection(
            edges=edges,
            page_info=PageInfo(
                has_next_page=has_next,
                end_cursor=edges[-1].cursor if edges else None
            )
        )
    ```

    **Caching Strategy:**
    - L1 Cache: In-memory LRU (100MB, 1min TTL)
    - L2 Cache: Redis (1GB, 5min TTL)
    - Cache key pattern: `gql:{query_hash}:{variables_hash}`
    - Cache invalidation: Event-driven via Pub/Sub
  </Tabs.Tab>
</Tabs>

### Real-time Data Streaming Flow

This flowchart shows the complete real-time data streaming architecture, from device data acquisition through WebSocket delivery to clients.

```mermaid
flowchart TB
    subgraph "Data Sources"
        D1[OpenBCI Device]
        D2[Emotiv Device]
        D3[Custom BCI]
        D4[Synthetic Device]
    end

    subgraph "Device Layer"
        DM[Device Manager]
        RB[Ring Buffer<br/>Per Device]
        DS[Data Sampler<br/>250-1000Hz]
        IM[Impedance Monitor]
    end

    subgraph "Processing Layer"
        AG[Aggregator<br/>Time Sync]
        FT[Filter<br/>Bandpass/Notch]
        WN[Windowing<br/>1-4s windows]
        FE[Feature Extraction<br/>Parallel]
    end

    subgraph "Transport Layer"
        PS[Pub/Sub Publisher]
        WS[WebSocket Server]
        SB[Stream Buffer<br/>Backpressure]
        CM[Connection Manager]
    end

    subgraph "Client Layer"
        WC1[WebSocket Client 1]
        WC2[WebSocket Client 2]
        WC3[WebSocket Client N]
        VZ[Visualizations]
    end

    subgraph "Storage Layer"
        BT[Bigtable<br/>Time-series]
        RD[Redis<br/>Hot Data]
        S3[Cloud Storage<br/>Raw Files]
    end

    D1 -->|USB/BLE| DM
    D2 -->|SDK| DM
    D3 -->|LSL| DM
    D4 -->|Internal| DM

    DM --> RB
    RB --> DS
    DS --> IM

    IM -->|Valid| AG
    IM -->|Invalid| DM

    AG --> FT
    FT --> WN
    WN --> FE

    FE --> PS
    FE --> WS
    FE --> BT

    PS -->|Topics| CM
    WS --> SB
    SB --> CM

    CM --> WC1
    CM --> WC2
    CM --> WC3

    WC1 --> VZ
    WC2 --> VZ
    WC3 --> VZ

    PS --> BT
    PS --> RD
    PS --> S3

    style D1 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D2 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D3 fill:#4285F4,stroke:#1a73e8,color:#fff
    style D4 fill:#4285F4,stroke:#1a73e8,color:#fff
    style DM fill:#34A853,stroke:#188038,color:#fff
    style AG fill:#FBBC04,stroke:#F9AB00,color:#000
    style PS fill:#EA4335,stroke:#C5221F,color:#fff
    style WS fill:#EA4335,stroke:#C5221F,color:#fff
    style BT fill:#9E9E9E,stroke:#616161,color:#fff
```

<Tabs items={['Streaming Architecture', 'WebSocket Protocol', 'Implementation']}>
  <Tabs.Tab>
    **Streaming Components:**

    | Component | Purpose | Capacity | Latency |
    |-----------|---------|----------|---------|
    | **Ring Buffer** | Per-device circular buffer | 10s of data | Less than 1ms write |
    | **Data Sampler** | Downsample/upsample to target rate | 250-1000 Hz | Less than 1ms |
    | **Aggregator** | Time-align multi-device streams | 32 devices | Less than 5ms |
    | **Filter** | Remove noise and artifacts | Real-time | Less than 2ms |
    | **Windowing** | Create analysis windows | 1-4s windows | Less than 1ms |
    | **Feature Extraction** | Compute real-time features | 100+ features | Less than 10ms |
    | **WebSocket Server** | Real-time client delivery | 1000+ clients | Less than 5ms |
    | **Stream Buffer** | Handle client backpressure | 1MB per client | Variable |

    **Data Flow Rates:**
    - Raw data: 32 channels × 1000 Hz × 4 bytes = 128 KB/s per device
    - Feature data: 100 features × 10 Hz × 4 bytes = 4 KB/s per device
    - WebSocket messages: ~100-1000 msg/s per client
    - Pub/Sub throughput: 10,000+ msg/s total
  </Tabs.Tab>

  <Tabs.Tab>
    **WebSocket Message Protocol:**

    ```typescript
    // Client -> Server Messages
    interface ClientMessage {
      type: "subscribe" | "unsubscribe" | "configure";
      payload: SubscribePayload | UnsubscribePayload | ConfigPayload;
    }

    interface SubscribePayload {
      streamType: "raw" | "features" | "predictions";
      deviceIds: string[];
      channels?: number[];
      sampleRate?: number;
    }

    // Server -> Client Messages
    interface ServerMessage {
      type: "data" | "event" | "error" | "ack";
      timestamp: number;
      payload: DataPayload | EventPayload | ErrorPayload;
    }

    interface DataPayload {
      deviceId: string;
      streamType: "raw" | "features" | "predictions";
      data: Float32Array | FeatureData | PredictionData;
      sequenceNumber: number;
    }

    interface EventPayload {
      eventType: "device_connected" | "device_disconnected" | "impedance_change";
      deviceId: string;
      details: any;
    }
    ```

    **Connection Lifecycle:**
    1. Client connects: `ws://api/v1/streaming/neural-data`
    2. Server sends: `{"type": "ack", "payload": {"connectionId": "..."}}`
    3. Client subscribes: `{"type": "subscribe", "payload": {...}}`
    4. Server streams data: `{"type": "data", "payload": {...}}`
    5. Client can configure filtering, sampling rate, etc.
    6. Server handles backpressure with buffering
    7. Clean disconnect or timeout handling
  </Tabs.Tab>

  <Tabs.Tab>
    **Streaming Implementation:**

    ```python
    class StreamingService:
        def __init__(self):
            self.connections: Dict[str, WebSocketConnection] = {}
            self.device_streams: Dict[str, DeviceStream] = {}
            self.buffer_size = 10000  # samples per device

        async def handle_connection(self, websocket: WebSocket):
            connection_id = str(uuid4())
            connection = WebSocketConnection(
                id=connection_id,
                websocket=websocket,
                subscriptions=set(),
                buffer=StreamBuffer(max_size=1024*1024)  # 1MB
            )

            self.connections[connection_id] = connection

            try:
                # Send acknowledgment
                await websocket.send_json({
                    "type": "ack",
                    "timestamp": time.time(),
                    "payload": {"connectionId": connection_id}
                })

                # Handle messages
                while True:
                    message = await websocket.receive_json()
                    await self._handle_client_message(connection, message)

            except WebSocketDisconnect:
                await self._cleanup_connection(connection_id)

        async def stream_data(self, device_id: str, data: np.ndarray):
            # Process through pipeline
            filtered = await self.filter_data(data)
            windowed = self.window_data(filtered)
            features = await self.extract_features(windowed)

            # Publish to subscribers
            tasks = []
            for conn_id, connection in self.connections.items():
                if device_id in connection.subscriptions:
                    tasks.append(
                        self._send_to_client(connection, {
                            "type": "data",
                            "timestamp": time.time(),
                            "payload": {
                                "deviceId": device_id,
                                "streamType": "features",
                                "data": features.tolist(),
                                "sequenceNumber": self._get_sequence()
                            }
                        })
                    )

            # Send concurrently with backpressure handling
            await asyncio.gather(*tasks, return_exceptions=True)

        async def _send_to_client(self, connection: WebSocketConnection, message: dict):
            # Check buffer space
            if connection.buffer.available_space() < len(message):
                # Apply backpressure
                await connection.buffer.wait_for_space()

            # Queue message
            connection.buffer.put(message)

            # Send if not already sending
            if not connection.is_sending:
                await self._flush_buffer(connection)
    ```

    **Performance Optimizations:**
    - Zero-copy buffers for raw data
    - SIMD operations for filtering
    - Parallel feature extraction
    - Message batching for efficiency
    - Automatic quality degradation under load
  </Tabs.Tab>
</Tabs>

### ML Model Lifecycle Flow

This flowchart shows the complete machine learning model lifecycle from data collection through deployment and monitoring, including retraining feedback loops.

```mermaid
flowchart TB
    subgraph "Data Collection"
        RAW[Raw Neural Data<br/>Sessions]
        LABEL[Labeled Events<br/>Clinical Annotations]
        QUALITY[Quality Metrics]
    end

    subgraph "Data Preparation"
        CLEAN[Data Cleaning<br/>Artifact Removal]
        SPLIT[Train/Val/Test<br/>80/10/10 Split]
        AUGMENT[Data Augmentation<br/>Synthetic Samples]
        FEATURE[Feature Engineering<br/>Domain Features]
    end

    subgraph "Model Development"
        EXPLORE[Model Exploration<br/>Architecture Search]
        TRAIN[Model Training<br/>Distributed GPU]
        VALIDATE[Validation<br/>Cross-validation]
        OPTIMIZE[Hyperparameter<br/>Optimization]
    end

    subgraph "Model Evaluation"
        METRICS[Performance Metrics<br/>Acc/Prec/Recall]
        CLINICAL_VAL[Clinical Validation<br/>Expert Review]
        BIAS[Bias Detection<br/>Fairness Analysis]
        INTERPRET[Interpretability<br/>SHAP/LIME]
    end

    subgraph "Model Registry"
        VERSION[Version Control<br/>Git LFS]
        METADATA[Model Metadata<br/>Training Config]
        ARTIFACT[Model Artifacts<br/>Weights/Config]
        APPROVE[Approval Workflow]
    end

    subgraph "Deployment"
        PACKAGE[Model Packaging<br/>Docker/ONNX]
        DEPLOY_TEST[Test Deployment<br/>Staging Env]
        CANARY[Canary Release<br/>5% Traffic]
        PROD[Production Deploy<br/>100% Traffic]
    end

    subgraph "Inference"
        SERVE[Model Serving<br/>TensorFlow Serving]
        CACHE_INF[Inference Cache<br/>Redis]
        BATCH[Batch Inference<br/>Offline Jobs]
        STREAM_INF[Stream Inference<br/>Real-time]
    end

    subgraph "Monitoring"
        PERF_MON[Performance Monitor<br/>Latency/Throughput]
        DRIFT[Drift Detection<br/>Data/Concept]
        ALERT[Alert System<br/>PagerDuty]
        FEEDBACK[User Feedback<br/>Clinical Outcomes]
    end

    RAW --> CLEAN
    LABEL --> CLEAN
    QUALITY --> CLEAN

    CLEAN --> SPLIT
    SPLIT --> AUGMENT
    AUGMENT --> FEATURE

    FEATURE --> EXPLORE
    EXPLORE --> TRAIN
    TRAIN --> VALIDATE
    VALIDATE --> OPTIMIZE
    OPTIMIZE --> TRAIN

    VALIDATE --> METRICS
    METRICS --> CLINICAL_VAL
    CLINICAL_VAL --> BIAS
    BIAS --> INTERPRET

    INTERPRET --> VERSION
    VERSION --> METADATA
    METADATA --> ARTIFACT
    ARTIFACT --> APPROVE

    APPROVE --> PACKAGE
    PACKAGE --> DEPLOY_TEST
    DEPLOY_TEST --> CANARY
    CANARY --> PROD

    PROD --> SERVE
    SERVE --> CACHE_INF
    SERVE --> BATCH
    SERVE --> STREAM_INF

    STREAM_INF --> PERF_MON
    BATCH --> PERF_MON
    PERF_MON --> DRIFT
    DRIFT --> ALERT
    ALERT --> FEEDBACK

    FEEDBACK -->|Retrain Trigger| RAW
    DRIFT -->|Retrain Trigger| RAW

    style RAW fill:#4285F4,stroke:#1a73e8,color:#fff
    style TRAIN fill:#34A853,stroke:#188038,color:#fff
    style PROD fill:#EA4335,stroke:#C5221F,color:#fff
    style DRIFT fill:#FBBC04,stroke:#F9AB00,color:#000
```

<Tabs items={['Training Pipeline', 'Deployment Strategy', 'Monitoring & Retraining']}>
  <Tabs.Tab>
    **Model Training Pipeline:**

    | Stage | Duration | Resources | Output |
    |-------|----------|-----------|--------|
    | **Data Collection** | Continuous | Storage: 10TB/month | Labeled datasets |
    | **Data Cleaning** | 2-4 hours | CPU: 32 cores | Clean dataset |
    | **Feature Engineering** | 1-2 hours | CPU: 16 cores | Feature matrices |
    | **Model Training** | 4-12 hours | GPU: 4x V100 | Trained model |
    | **Hyperparameter Opt** | 8-24 hours | GPU: 8x V100 | Best parameters |
    | **Validation** | 1-2 hours | CPU: 8 cores | Metrics report |
    | **Clinical Review** | 1-3 days | Human experts | Approval/feedback |

    **Supported Model Types:**
    ```python
    MODEL_ARCHITECTURES = {
        "classification": {
            "EEGNet": "Compact CNN for EEG",
            "DeepConvNet": "Deep CNN architecture",
            "ShallowConvNet": "Shallow CNN for online",
            "LSTM-FCN": "LSTM + Fully Convolutional",
            "Transformer": "Attention-based model"
        },
        "regression": {
            "CNN-LSTM": "Feature prediction",
            "TCN": "Temporal Convolutional Network",
            "WaveNet": "Dilated convolutions"
        },
        "anomaly_detection": {
            "Autoencoder": "Reconstruction-based",
            "VAE": "Variational Autoencoder",
            "GAN": "Generative Adversarial Network"
        }
    }
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Deployment Process:**

    ```yaml
    stages:
      - name: Model Validation
        steps:
          - Unit tests for model code
          - Integration tests with pipeline
          - Performance benchmarks
          - Security scanning

      - name: Containerization
        steps:
          - Build Docker image
          - Optimize for inference (quantization)
          - Create ONNX export
          - Push to Artifact Registry

      - name: Staging Deployment
        steps:
          - Deploy to staging cluster
          - Run smoke tests
          - Validate against test data
          - Monitor for 24 hours

      - name: Canary Release
        steps:
          - Deploy to 5% of traffic
          - Monitor key metrics
          - Compare with baseline
          - Gradual rollout to 25%, 50%, 100%

      - name: Production Monitoring
        steps:
          - Real-time performance tracking
          - Error rate monitoring
          - Resource utilization
          - User feedback collection
    ```

    **Rollback Strategy:**
    - Automatic rollback on error rate > 5%
    - Manual rollback capability
    - Previous 3 versions kept hot
    - Zero-downtime rollback
  </Tabs.Tab>

  <Tabs.Tab>
    **Model Monitoring Metrics:**

    | Metric | Threshold | Action |
    |--------|-----------|--------|
    | **Inference Latency** | > 100ms (p99) | Scale up replicas |
    | **Error Rate** | > 1% | Alert + investigate |
    | **Data Drift** | KL divergence > 0.1 | Trigger retraining |
    | **Concept Drift** | Accuracy drop > 5% | Immediate retraining |
    | **Resource Usage** | > 80% | Auto-scale |
    | **Cache Hit Rate** | < 60% | Optimize caching |

    **Retraining Triggers:**
    ```python
    class RetrainingTrigger:
        def __init__(self):
            self.triggers = {
                "scheduled": CronTrigger("0 0 * * 0"),  # Weekly
                "performance": ThresholdTrigger(
                    metric="accuracy",
                    threshold=0.85,
                    direction="below"
                ),
                "drift": DriftTrigger(
                    method="kolmogorov_smirnov",
                    threshold=0.05
                ),
                "data_volume": VolumeTrigger(
                    new_samples=10000,
                    min_interval_days=7
                ),
                "clinical_feedback": ManualTrigger(
                    approval_required=True
                )
            }

        async def check_triggers(self):
            for name, trigger in self.triggers.items():
                if await trigger.should_fire():
                    await self.initiate_retraining(name)
    ```

    **Continuous Learning Pipeline:**
    - Incremental learning for online adaptation
    - Active learning for efficient labeling
    - Transfer learning from pre-trained models
    - Federated learning for privacy-preserving updates
  </Tabs.Tab>
</Tabs>

### System Error Recovery State Machine

This comprehensive state diagram shows the system-wide error detection, handling, and recovery mechanisms across all services, ensuring high availability and data integrity.

```mermaid
stateDiagram-v2
    [*] --> Normal

    Normal --> Degraded: performance_degradation
    Normal --> Error: error_detected
    Normal --> Maintenance: scheduled_maintenance

    Degraded --> Normal: performance_restored
    Degraded --> Error: error_threshold_exceeded
    Degraded --> Critical: cascade_failure

    Error --> Recovering: auto_recovery_initiated
    Error --> Degraded: partial_recovery
    Error --> Critical: recovery_failed
    Error --> Investigation: manual_intervention

    Recovering --> Normal: recovery_successful
    Recovering --> Degraded: partial_recovery
    Recovering --> Critical: recovery_timeout

    Critical --> Emergency: data_loss_risk
    Critical --> Recovering: emergency_recovery
    Critical --> Failed: unrecoverable_error

    Emergency --> Recovering: backup_restoration
    Emergency --> Failed: backup_corrupted

    Investigation --> Recovering: issue_resolved
    Investigation --> Maintenance: repair_required

    Maintenance --> Normal: maintenance_complete
    Maintenance --> Error: maintenance_failed

    Failed --> [*]: system_restart

    state Normal {
        [*] --> Monitoring
        Monitoring --> HealthCheck: interval_5s
        HealthCheck --> Monitoring: all_healthy
        HealthCheck --> [*]: degradation_detected
    }

    state Error {
        [*] --> Identifying
        Identifying --> Categorizing: error_identified
        Categorizing --> Logging: category_assigned
        Logging --> Notifying: logged_to_system
        Notifying --> [*]: stakeholders_notified
    }

    state Recovering {
        [*] --> Analyzing
        Analyzing --> Planning: root_cause_found
        Planning --> Executing: recovery_plan_ready
        Executing --> Validating: actions_complete
        Validating --> [*]: validation_result
    }

    state Critical {
        [*] --> Isolating
        Isolating --> Containing: affected_components_identified
        Containing --> Mitigating: damage_contained
        Mitigating --> [*]: immediate_risk_reduced
    }
```

<Tabs items={['Error Categories', 'Recovery Strategies', 'Implementation']}>
  <Tabs.Tab>
    **System Error Categories:**

    | Category | Severity | Examples | Auto-Recovery |
    |----------|----------|----------|---------------|
    | **Device Errors** | Medium | Connection lost, High impedance | Yes (retry 3x) |
    | **Data Errors** | High | Corruption, Loss, Invalid format | Partial |
    | **Service Errors** | Medium | Timeout, 5xx errors, OOM | Yes (restart) |
    | **Network Errors** | Low | Latency, Packet loss | Yes (reroute) |
    | **Storage Errors** | Critical | Disk full, Write failure | No |
    | **Security Errors** | Critical | Auth failure, Breach attempt | No |
    | **Processing Errors** | Medium | Pipeline stall, ML failure | Yes (retry) |
    | **Database Errors** | High | Connection pool, Deadlock | Yes (reconnect) |

    **Error Detection Methods:**
    ```python
    class ErrorDetector:
        def __init__(self):
            self.monitors = {
                "health_checks": HealthMonitor(interval=5),
                "metrics": MetricsMonitor(prometheus_client),
                "logs": LogMonitor(patterns=ERROR_PATTERNS),
                "traces": TraceMonitor(opentelemetry),
                "synthetic": SyntheticMonitor(probes=ENDPOINTS)
            }

        async def detect_anomalies(self):
            # Statistical anomaly detection
            metrics = await self.metrics.get_time_series()
            anomalies = self.detector.find_anomalies(
                metrics,
                method="isolation_forest",
                contamination=0.1
            )

            # Pattern-based detection
            log_errors = await self.logs.search_patterns(
                patterns=[
                    r"ERROR|CRITICAL|FATAL",
                    r"Exception|Traceback",
                    r"timeout|refused|failed"
                ]
            )

            # Threshold-based detection
            thresholds = {
                "error_rate": 0.01,  # 1%
                "latency_p99": 1000,  # 1s
                "cpu_usage": 0.8,     # 80%
                "memory_usage": 0.9,  # 90%
            }
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Recovery Strategy Matrix:**

    | Error Type | Strategy | Timeout | Fallback |
    |------------|----------|---------|----------|
    | **Transient** | Exponential backoff | 30s | Circuit breaker |
    | **Resource** | Auto-scaling | 60s | Load shedding |
    | **Corruption** | Restore from backup | 5min | Read-only mode |
    | **Service** | Rolling restart | 2min | Graceful degradation |
    | **Network** | Route failover | 10s | Cache serving |
    | **Database** | Connection retry | 30s | Read replica |
    | **Critical** | Full system recovery | 15min | Disaster recovery |

    **Circuit Breaker Pattern:**
    ```python
    class CircuitBreaker:
        def __init__(self, failure_threshold=5, timeout=60):
            self.failure_threshold = failure_threshold
            self.timeout = timeout
            self.failure_count = 0
            self.last_failure_time = None
            self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

        async def call(self, func, *args, **kwargs):
            if self.state == "OPEN":
                if self._should_attempt_reset():
                    self.state = "HALF_OPEN"
                else:
                    raise CircuitOpenError("Circuit breaker is OPEN")

            try:
                result = await func(*args, **kwargs)
                self._on_success()
                return result
            except Exception as e:
                self._on_failure()
                raise

        def _on_success(self):
            self.failure_count = 0
            self.state = "CLOSED"

        def _on_failure(self):
            self.failure_count += 1
            self.last_failure_time = time.time()

            if self.failure_count >= self.failure_threshold:
                self.state = "OPEN"
                self._trigger_alert()
    ```
  </Tabs.Tab>

  <Tabs.Tab>
    **Error Recovery Implementation:**

    ```python
    class SystemRecoveryManager:
        def __init__(self):
            self.state = SystemState.NORMAL
            self.recovery_strategies = {
                ErrorType.DEVICE: DeviceRecoveryStrategy(),
                ErrorType.SERVICE: ServiceRecoveryStrategy(),
                ErrorType.DATA: DataRecoveryStrategy(),
                ErrorType.NETWORK: NetworkRecoveryStrategy()
            }

        async def handle_error(self, error: SystemError):
            # Update system state
            await self.transition_to_error_state(error)

            # Log and notify
            await self.log_error(error)
            await self.notify_stakeholders(error)

            # Attempt recovery
            strategy = self.recovery_strategies.get(error.type)
            if strategy and error.severity < Severity.CRITICAL:
                try:
                    await strategy.recover(error)
                    await self.transition_to_normal_state()
                except RecoveryFailure:
                    await self.escalate_to_critical(error)
            else:
                await self.initiate_emergency_protocol(error)

        async def health_check_loop(self):
            while True:
                try:
                    health = await self.check_all_services()

                    if health.degraded_services:
                        await self.transition_to_degraded_state(
                            health.degraded_services
                        )

                    if health.failed_services:
                        await self.handle_service_failures(
                            health.failed_services
                        )

                    await asyncio.sleep(5)  # 5-second interval

                except Exception as e:
                    logger.critical(f"Health check failed: {e}")
                    await self.initiate_emergency_protocol(
                        SystemError(
                            type=ErrorType.MONITOR,
                            severity=Severity.CRITICAL,
                            message=str(e)
                        )
                    )
    ```

    **Graceful Degradation Levels:**
    1. **Level 1**: Disable non-essential features
    2. **Level 2**: Read-only mode for data
    3. **Level 3**: Serve from cache only
    4. **Level 4**: Static error page
    5. **Level 5**: Complete shutdown
  </Tabs.Tab>
</Tabs>
